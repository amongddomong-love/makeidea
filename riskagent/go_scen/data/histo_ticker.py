# -*- coding: utf-8 -*-
"""
ëª©ì :
- ë¸”ë£¸ë²„ê·¸ xbbgë¡œ ì£¼ìš” ì§€í‘œë¥¼ ì¡°íšŒí•˜ê³ , 1ì¼/10ì¼ ë³€í™”ìœ¨(ë˜ëŠ” bp/pp) ê¸°ë°˜ ì„ê³„ìˆ˜ì¤€ ì´ˆê³¼ ì—¬ë¶€ë¥¼ ì ê²€
- íŠ¹ì • í•­ëª©ì€ ì›”í‰ê· (MTD/PrevM/3M-ago) ê¸°ì¤€ìœ¼ë¡œ ìŠ¤í”„ë ˆë“œ/ë³€ë™ë¥  ì„ê³„ì¹˜ ì ê²€
- âš ï¸ "í˜„ì¬ ê°’ì´ ë‚˜ì˜¤ëŠ” ì§€í‘œë§Œ" alertsì— ë°˜ì˜ (ë°ì´í„° ë¯¸ìˆ˜ê¸‰ ì‹œ í•´ë‹¹ ë¸”ë¡ì€ ì½”ë“œì—ì„œ ì£¼ì„ ì²˜ë¦¬ ì˜ˆì‹œë¥¼ ë‚¨ê¹€)
- ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼(ìš”ì•½ alerts + ì›ì‹œ raw_data)ë¡œ ì €ì¥

í™˜ê²½ ìœ ì˜:
- Bloomberg Desktop + xbbg (í„°ë¯¸ë„ ë¡œê·¸ì¸ ìƒíƒœ) í•„ìš”
- pip: xbbg, pandas, numpy, openpyxl ë“±
- Windows ê²½ë¡œëŠ” ë°˜ë“œì‹œ pathlib.Path ì‚¬ìš© ë˜ëŠ” / ìŠ¬ë˜ì‹œ

ì„ê³„ìˆ˜ì¤€ ìš”ì•½:
- ì›í™”ê¸ˆë¦¬(êµ­ê³ 3Y): 1ì¼ Â±15bp, 10ì¼ Â±50bp
- ì›í™”ê¸ˆë¦¬(êµ­ê³ 10Y): 1ì¼ Â±15bp, 10ì¼ Â±45bp
- ì™¸í™”ê¸ˆë¦¬(ë¯¸10Y/Term SOFR 6M): ìµœê·¼ 3ê°œì›” í‰ê·  ëŒ€ë¹„ Â±100bp
- KRWUSD í™˜ìœ¨: 1ì¼ Â±2%, 10ì¼ Â±5%
- KOSPI: 1ì¼ -3.5%, 10ì¼ -10% (í•˜ë½ ë°©í–¥ë§Œ)
- VKOSPI: 1ì¼ +5%p, 10ì¼ +10%p (ìƒìŠ¹ ë°©í–¥ë§Œ)
- USDKRW 1Y IV: 1ì¼ Â±5%p, 10ì¼ Â±10%p
- ì™¸í™” ì›”í‰ê·  ì¥ë‹¨ê¸° ê¸ˆë¦¬ì°¨: (SOFR OIS 1Y - TSFR 1M) MTD í‰ê·  â‰¥ +150bp
- KR 1Y - ê¸°ì¤€ê¸ˆë¦¬: 5ì˜ì—…ì¼ ì—°ì† < -24bp
- ê¸°ì¤€ê¸ˆë¦¬ - ì½œê¸ˆë¦¬: > +40bp (ë ˆë²¨)
- Term SOFR 3M: MTD-PrevM ì ˆëŒ€ë³€í™” > 75bp
- JPY 3M TIBOR: MTD-PrevM ì ˆëŒ€ë³€í™” > 25bp
- í•œêµ­ 5Y CDS: PrevM ëŒ€ë¹„ +100bp 3ì˜ì—…ì¼ ì—°ì† / M-3 ëŒ€ë¹„ +200bp 3ì˜ì—…ì¼ ì—°ì†
- KR Term Spread (10Y-3Y): ì—­ì „(â‰¤0bp) 5ì˜ì—…ì¼ ì—°ì†
- êµ­ê°€ë³„ CDS 17ê°œêµ­: ì „ì›” í‰ê·  ëŒ€ë¹„ +30% ìƒìŠ¹
- (íšŒì‚¬ì±„/êµ­ê³ ) 3Y ë¹„ìœ¨: ì „ì›”í‰ê·  ëŒ€ë¹„ +16% ìƒìŠ¹
- ì›”í‰ê·  ì¥ë‹¨ê¸° (ê¸ˆìœµì±„1Y - CD3M): MTD ìŠ¤í”„ë ˆë“œ â‰¥ +70bp
- (ê¸ˆìœµì±„1Y AAA - KTB1Y): 5ì˜ì—…ì¼ ì—°ì† â‰¥ +50bp
- S&P500: 1ì¼ â‰¤ -3%, 10ì¼ â‰¤ -12%
- EuroStoxx50: 1ì¼ |Î”| â‰¥ 3%, 10ì¼ â‰¤ -12%
"""

from pathlib import Path  # âœ… ìœˆë„ìš°ì—ì„œë„ ì•ˆì „í•œ ê²½ë¡œ ì²˜ë¦¬
from datetime import timedelta
import numpy as np
import pandas as pd

# Bloomberg
try:
    from xbbg import blp
except ImportError as e:
    raise SystemExit(
        "xbbgê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ìŒì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”:\n"
        "    pip install xbbg pandas numpy openpyxl\n"
        f"ì›ë³¸ ì—ëŸ¬: {e}"
    )

# -----------------------------
# 0) ê³µí†µ ì„¤ì •
# -----------------------------
TODAY = pd.Timestamp.today(tz="Asia/Seoul").date()
# ì›”í‰ê· /ì—°ì†ì¼ íŒì • ë“±ì„ ìœ„í•´ ì¶©ë¶„í•œ ê³¼ê±°ì¹˜ í™•ë³´ (ì•½ 14ê°œì›”)
START_DATE = (pd.Timestamp(TODAY) - timedelta(days=420)).strftime("%Y-%m-%d")
END_DATE = pd.Timestamp(TODAY).strftime("%Y-%m-%d")

# ğŸ’¾ ì €ì¥ ê²½ë¡œ (ì‹¤í–‰ í´ë”)
output_path = Path(r"C:/Users/amongpapa/chartup/raw_data") / f"risk_thresholds_{pd.Timestamp(TODAY).strftime('%Y%m%d')}.xlsx"

# -----------------------------
# 1) í‹°ì»¤ ë§µ
# -----------------------------
TICKERS = {
    # KTB / KR rates
    "KR1Y":         "SKTB1YAY Index",
    "KR3Y":         "SKTB3YAY Index",
    "KR10Y":        "SKTB10YY Index",

    # UST & SOFR
    "US10Y":        "USGG10YR Index",
    "TSFR6M":       "TSFR6M Index",
    "TSFR3M":       "TSFR3M Index",
    "TSFR1M":       "TSFR1M Index",
    "SOFR_OIS_1Y":  "USOSFR1 BGN Curncy",   # âœ… íŒ€ì¥ë‹˜ ì§€ì •

    # FX / Equities / Vol
    "USDKRW":       "USDKRW Curncy",
    "KOSPI":        "KOSPI Index",
    "VKOSPI":       "VKOSPI Index",
    "KRW_IV1Y":     "USDKRWV1Y BGN Curncy",

    # KR policy & money
    "KRBASERATE":   "KORP7D Index",         # âœ… í•œêµ­ ê¸°ì¤€ê¸ˆë¦¬
    "KRCALL":       "KOCR Index",           # âœ… ì‹œì¥ ì½œê¸ˆë¦¬
    "KR_CD3M":      "KWCDC CMPN Curncy",    # âœ… CD 3ê°œì›”ë¬¼
    "KR_FIN1Y_AAA": "KRFN1YAA Index",       # ê¸ˆìœµì±„ 1ë…„ AAA
    "KR_CORP3Y_AA-":"KRCORP3YAA- Index",    # íšŒì‚¬ì±„ 3ë…„ AA-

    # Other markets
    "SPX":          "SPX Index",
    "SX5E":         "SX5E Index",

    # JPY money market
    "JPY_TIBOR3M":  "TI0003M Index",        # âœ… JPY 3M TIBOR

    # FRA-OIS (í™˜ê²½ì— ë”°ë¼ í‹±ì»¤ ìƒì´í•  ìˆ˜ ìˆìŒ)
    "US_FRAOIS_3M": "USSFRAOIS Index",      # âš ï¸ í•„ìš”ì‹œ êµì²´/ë¹„í™œì„±
}

# êµ­ê°€ë³„ 5Y CDS (D14)
CDS_TICKERS = {
    "Korea":          "KOREA CDS USD SR 5Y D14 Curncy",   # í•œêµ­
    "United States":  "US CDS EUR SR 5Y D14 Curncy",
    "Japan":          "JGB CDS USD SR 5Y D14 Curncy",
    "China":          "CHINAGOV CDS USD SR 5Y D14 Curncy",
    "Vietnam":        "VIETNM CDS USD SR 5Y D14 Curncy",
    "Kazakhstan":     "KAZAKS CDS USD SR 5Y D14 Curncy",
    "Germany":        "GERMAN CDS USD SR 5Y D14 Curncy",
    "United Kingdom": "UK CDS USD SR 5Y D14 Curncy",
    "India":          "INDIA CDS USD SR 5Y D14 Curncy",
    "Mexico":         "MEX CDS USD SR 5Y D14 Curncy",
    "Indonesia":      "INDON CDS USD SR 5Y D14 Curncy",
    "TÃ¼rkiye":        "TURKEY CDS USD SR 5Y D14 Curncy",
    "Canada":         "CANPAC CDS USD SR 5Y D14 Curncy",
    "Hong Kong":      "HONGK CDS USD SR 5Y D14 Curncy",
    "Australia":      "AUSTLA CDS USD SR 5Y D14 Curncy",
    "Philippines":    "PHILIP CDS USD SR 5Y D14 Curncy",
    "Singapore":      "SINGP CDS USD SR 5Y D14 Curncy",    # âœ… ì¶”ê°€ ê°€ëŠ¥
    "UAE":            "DPWDU CDS USD SR 5Y D14 Curncy",    # âœ… ìš”ì²­ì‚¬í•­ ë°˜ì˜(ê¸°ì—…ê³„ì—´)
}

# -----------------------------
# 2) í•„ë“œ ìš°ì„ ìˆœìœ„
# -----------------------------
# - ê¸ˆë¦¬: YLD_YTM_MID > YLD_YTM_LAST > PX_LAST
# - ì§€ìˆ˜/í™˜ìœ¨/ë³€ë™ì„±: PX_LAST
# - â˜… CDS: LAST_PRICE > MID > PX_LAST  (â€» í•µì‹¬ ìˆ˜ì •)
FIELD_PREFS = {}
for k in TICKERS:
    if k in ["KR1Y","KR3Y","KR10Y"]:
        FIELD_PREFS[k] = ["YLD_YTM_MID", "YLD_YTM_LAST", "PX_LAST"]
    elif k in ["US10Y"]:
        FIELD_PREFS[k] = ["PX_LAST", "YLD_YTM_MID"]
    elif k in ["TSFR6M","TSFR3M","TSFR1M","USDKRW","KOSPI","VKOSPI","KRW_IV1Y","SPX","SX5E"]:
        FIELD_PREFS[k] = ["PX_LAST"]
    elif k in ["SOFR_OIS_1Y"]:
        FIELD_PREFS[k] = ["PX_LAST", "YLD_YTM_MID", "LAST_PRICE"]
    elif k in ["KRBASERATE","KRCALL","KR_CD3M","JPY_TIBOR3M","US_FRAOIS_3M"]:
        FIELD_PREFS[k] = ["PX_LAST", "LAST_PRICE"]
    elif k in ["KR_FIN1Y_AAA","KR_CORP3Y_AA-"]:
        FIELD_PREFS[k] = ["PX_LAST", "YLD_YTM_MID", "LAST_PRICE"]
    else:
        FIELD_PREFS[k] = ["PX_LAST"]

# â˜… CDS ì „ìš©: (ì´ì „: PX_MIDâ†’PX_LAST) â†’ (ë³€ê²½: LAST_PRICEâ†’MIDâ†’PX_LAST)
for name in CDS_TICKERS:
    FIELD_PREFS[name] = ["LAST_PRICE", "MID", "PX_LAST"]  # â˜… ìˆ˜ì •

# â˜… BDH í˜¸ì¶œ ì‹œ ëˆ„ë½ ë°©ì§€ë¥¼ ìœ„í•´ í•­ìƒ í¬í•¨í•  Fallback í•„ë“œ
FALLBACK_FIELDS = {"LAST_PRICE", "MID", "PX_LAST", "PX_MID", "YLD_YTM_MID", "YLD_YTM_LAST"}  # â˜… ìˆ˜ì •

# -----------------------------
# 3) ì„ê³„ê°’
# -----------------------------
THRESHOLDS = {
    "KR3Y_1d_bp": 15,   "KR3Y_10d_bp": 50,
    "KR10Y_1d_bp": 15,  "KR10Y_10d_bp": 45,
    "USFX_1d_pct": 2.0, "USFX_10d_pct": 5.0,
    "KOSPI_1d_down_pct": -3.5, "KOSPI_10d_down_pct": -10.0,
    "VKOSPI_1d_up_pp": 5.0,    "VKOSPI_10d_up_pp": 10.0,
    "KRWIV_1d_pp": 5.0, "KRWIV_10d_pp": 10.0,
    "G3M_dev_bp": 100.0,

    # ì¶”ê°€ ì„ê³„
    "SPREAD_SOFR1M_vs_OIS1Y_MTD_bp": 150.0,
    "KR_1Y_minus_BASE_5d_level_bp": -24.0,
    "BASE_minus_CALL_bp": 40.0,
    "TSFR3M_prevM_abs_bp": 75.0,
    "JPY_TIBOR3M_prevM_abs_bp": 25.0,
    "KR5YCDS_prevM_bp_3d": 100.0,
    "KR5YCDS_M3ago_bp_3d": 200.0,
    "KR_10Y_3Y_inversion_5d": 5,
    "CDS_prevM_pct_up": 30.0,
    "CorpAAminus_KTB3Y_ratio_prevM_pct": 16.0,
    "Fin1Y_minus_CD3M_MTD_bp": 70.0,
    "Fin1YAAA_minus_KTB1Y_5d_bp": 50.0,
    "SPX_1d_down_pct": -3.0, "SPX_10d_down_pct": -12.0,
    "SX5E_1d_abs_pct": 3.0,  "SX5E_10d_down_pct": -12.0,
    "FRAOIS_prevM_bp": 30.0,
}

# -----------------------------
# 4) ìœ í‹¸ í•¨ìˆ˜
# -----------------------------
def fetch_hist_with_field_prefs(ticker_map, cds_map, field_prefs, start_date, end_date):
    """
    ë©€í‹° í•„ë“œë¡œ BDH ì¡°íšŒ í›„, ê°€ìš©ì„±ì´ ê°€ì¥ ì¢‹ì€ í•„ë“œë¥¼ ì±„íƒí•˜ì—¬ ë‹¨ì¼ ì‹œê³„ì—´ë¡œ ë³‘í•©
    ë°˜í™˜: DataFrame(index=Date, columns=keys)
    """
    all_pairs = list(ticker_map.items()) + list(cds_map.items())
    # â˜… ìš”ì²­ í•„ë“œ = ìš°ì„ ìˆœìœ„ í•„ë“œ í•©ì§‘í•© + Fallback (ëˆ„ë½ ë°©ì§€)
    prefs_fields = {fld for prefs in field_prefs.values() for fld in prefs}
    all_fields = sorted(prefs_fields | FALLBACK_FIELDS)  # â˜… ìˆ˜ì •

    raw = blp.bdh(
        tickers=[t for _, t in all_pairs],
        flds=all_fields,
        start_date=start_date,
        end_date=end_date,
        Per="D",
        Fill="P",
    )

    # ì¼ë¶€ í™˜ê²½ì—ì„œ ë‹¨ì¼ ì¢…ëª©/í•„ë“œ ì¡°í•©ì¼ ë•Œ MultiIndexê°€ ì•„ë‹ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°©ì–´
    if not isinstance(raw.columns, pd.MultiIndex):
        raw.columns = pd.MultiIndex.from_product([[all_pairs[0][1]], all_fields])

    raw.index = pd.to_datetime(raw.index, errors="coerce")
    raw = raw.sort_index()

    out = {}

    # ê³µí†µ fallback (ì¼ë°˜ìì‚°): PX_LAST â†’ LAST_PRICE â†’ PX_MID â†’ MID
    default_general = ["PX_LAST", "LAST_PRICE", "PX_MID", "MID"]
    # ê³µí†µ fallback (CDS): LAST_PRICE â†’ MID â†’ PX_LAST
    default_cds = ["LAST_PRICE", "MID", "PX_LAST"]

    # ì¼ë°˜ í‹±ì»¤
    for key, bb_ticker in ticker_map.items():
        prefs = field_prefs.get(key, default_general)  # â˜… ìˆ˜ì •: ì¼ë°˜ë„ ë³´ê°•
        ser = None
        for fld in prefs:
            if (bb_ticker, fld) in raw.columns:
                tmp = pd.to_numeric(raw[(bb_ticker, fld)], errors="coerce").ffill().bfill()
                if tmp.notna().sum() > 0:
                    ser = tmp
                    # print(f"[{key}] ì‚¬ìš© í•„ë“œ: {fld}")  # ë””ë²„ê·¸ìš©
                    break
        out[key] = ser if ser is not None else pd.Series(index=raw.index, dtype=float)

    # CDS(êµ­ê°€ëª… í‚¤)
    for name, bb_ticker in cds_map.items():
        prefs = field_prefs.get(name, default_cds)     # â˜… ìˆ˜ì •: CDS ê¸°ë³¸ê°’ ê³ ì •
        ser = None
        for fld in prefs:
            if (bb_ticker, fld) in raw.columns:
                tmp = pd.to_numeric(raw[(bb_ticker, fld)], errors="coerce").ffill().bfill()
                if tmp.notna().sum() > 0:
                    ser = tmp
                    # print(f"[CDS:{name}] ì‚¬ìš© í•„ë“œ: {fld}")  # ë””ë²„ê·¸ìš©
                    break
        out[name] = ser if ser is not None else pd.Series(index=raw.index, dtype=float)

    df = pd.DataFrame(out)
    df.index = pd.to_datetime(df.index, errors="coerce")
    return df

def last_value(series):
    s = series.dropna()
    return float(s.iloc[-1]) if len(s) else np.nan

def bp_change(series, days=1):
    s = series.dropna()
    if len(s) <= days:
        return np.nan
    return float((s.iloc[-1] - s.iloc[-1 - days]) * 100.0)

def pct_change(series, days=1):
    s = series.dropna()
    if len(s) <= days:
        return np.nan
    prev = s.iloc[-1 - days]
    if prev == 0 or np.isnan(prev):
        return np.nan
    return float((s.iloc[-1] / prev - 1.0) * 100.0)

def pp_change(series, days=1):
    s = series.dropna()
    if len(s) <= days:
        return np.nan
    return float(s.iloc[-1] - s.iloc[-1 - days])

def trailing_3m_avg(series):
    """ìµœê·¼ 3ê°œì›”(ì˜ì—…ì¼ ì•½ 63ê°œ) í‰ê· """
    s = series.dropna()
    if len(s) < 10:
        return np.nan
    window = min(63, len(s))
    return float(s.iloc[-window:].mean())

def month_avg(series, months_ago=0):
    """
    ë‹¬ë ¥ì›” í‰ê·  (months_ago=0: ë‹¹ì›”, 1: ì „ì›”, 3: 3ê°œì›”ì „)
    - ì¸ë±ìŠ¤ê°€ datetime ë³€í™˜ ê°€ëŠ¥í•œì§€ ì•ˆì „í•˜ê²Œ ì²´í¬
    """
    s = series.dropna()
    if s.empty:
        return np.nan
    dt = pd.to_datetime(s.index, errors="coerce")
    valid = ~pd.isna(dt)
    s, dt = s[valid], dt[valid]
    if len(s) == 0:
        return np.nan
    last_dt = dt.max()
    target = last_dt - pd.DateOffset(months=months_ago)
    mask = (dt.year == target.year) & (dt.month == target.month)
    if not mask.any():
        return np.nan
    # â˜… boolean mask ì¸ë±ì‹± ì•ˆì •í™”
    return float(s[mask].mean())  # â˜… ìˆ˜ì •

def consec_last_n(bool_series, n):
    """ë§ˆì§€ë§‰ Nì˜ì—…ì¼ ëª¨ë‘ True ì¸ì§€"""
    s = bool_series.dropna().astype(bool)
    if len(s) < n:
        return False
    return bool(s.iloc[-n:].all())

def has_data(series, min_points=5):
    """ë°ì´í„° ìœ íš¨ì„± ì²´í¬: ê²°ì¸¡ ì œê±° í›„ ìµœì†Œ ê°œìˆ˜ í™•ë³´"""
    return series.dropna().shape[0] >= min_points

# -----------------------------
# 5) ë°ì´í„° ìˆ˜ì§‘
# -----------------------------
hist = fetch_hist_with_field_prefs(
    ticker_map=TICKERS,
    cds_map=CDS_TICKERS,
    field_prefs=FIELD_PREFS,
    start_date=START_DATE,
    end_date=END_DATE,
)

# series ì ‘ê·¼ìš© ë§µ êµ¬ì„± (í‚¤: TICKERS/êµ­ê°€ëª…)
series_map = {}
for k in TICKERS.keys():
    if k in hist.columns:
        series_map[k] = hist[k]
for name in CDS_TICKERS.keys():
    if name in hist.columns:
        series_map[name] = hist[name]

# -----------------------------
# 6) ì„ê³„ ë¡œì§
# -----------------------------
rows = []

# ---- (A) ì›í™”ê¸ˆë¦¬ - êµ­ê³  3ë…„ ----
if has_data(series_map.get("KR3Y", pd.Series(dtype=float))):
    kr3y_1d = bp_change(series_map["KR3Y"], 1)
    kr3y_10d = bp_change(series_map["KR3Y"], 10)
    rows.append({
        "metric": "KR 3Y KTB Yield",
        "ticker": TICKERS["KR3Y"],
        "latest": last_value(series_map["KR3Y"]),
        "chg_1d": f"{kr3y_1d:.1f}bp" if pd.notna(kr3y_1d) else np.nan,
        "threshold_1d": "Â±15bp",
        "breach_1d": (abs(kr3y_1d) >= THRESHOLDS["KR3Y_1d_bp"]) if pd.notna(kr3y_1d) else np.nan,
        "chg_10d": f"{kr3y_10d:.1f}bp" if pd.notna(kr3y_10d) else np.nan,
        "threshold_10d": "Â±50bp",
        "breach_10d": (abs(kr3y_10d) >= THRESHOLDS["KR3Y_10d_bp"]) if pd.notna(kr3y_10d) else np.nan,
        "note": "ì›í™” 3Y: ìˆ˜ìµë¥  bp ê¸°ì¤€",
    })

# ---- (B) ì›í™”ê¸ˆë¦¬ - êµ­ê³  10ë…„ ----
if has_data(series_map.get("KR10Y", pd.Series(dtype=float))):
    kr10y_1d = bp_change(series_map["KR10Y"], 1)
    kr10y_10d = bp_change(series_map["KR10Y"], 10)
    rows.append({
        "metric": "KR 10Y KTB Yield",
        "ticker": TICKERS["KR10Y"],
        "latest": last_value(series_map["KR10Y"]),
        "chg_1d": f"{kr10y_1d:.1f}bp" if pd.notna(kr10y_1d) else np.nan,
        "threshold_1d": "Â±15bp",
        "breach_1d": (abs(kr10y_1d) >= THRESHOLDS["KR10Y_1d_bp"]) if pd.notna(kr10y_1d) else np.nan,
        "chg_10d": f"{kr10y_10d:.1f}bp" if pd.notna(kr10y_10d) else np.nan,
        "threshold_10d": "Â±45bp",
        "breach_10d": (abs(kr10y_10d) >= THRESHOLDS["KR10Y_10d_bp"]) if pd.notna(kr10y_10d) else np.nan,
        "note": "ì›í™” 10Y: ìˆ˜ìµë¥  bp ê¸°ì¤€",
    })

# ---- (C) ë¯¸10Y: 3M í‰ê·  ëŒ€ë¹„ Â±100bp ----
if has_data(series_map.get("US10Y", pd.Series(dtype=float))):
    us10y_last = last_value(series_map["US10Y"])
    us10y_3m = trailing_3m_avg(series_map["US10Y"])
    dev_bp = (us10y_last - us10y_3m) * 100.0 if pd.notna(us10y_last) and pd.notna(us10y_3m) else np.nan
    rows.append({
        "metric": "US 10Y vs 3M Avg",
        "ticker": TICKERS["US10Y"],
        "latest": us10y_last,
        "breach_3m": (abs(dev_bp) >= THRESHOLDS["G3M_dev_bp"]) if pd.notna(dev_bp) else np.nan,
        "note": f"3M avg={us10y_3m:.4f}, dev={dev_bp:.1f}bp; ì„ê³„Â±{THRESHOLDS['G3M_dev_bp']}bp" if pd.notna(dev_bp) else "ë°ì´í„° ë¶€ì¡±",
    })

# ---- (D) TSFR 6M: 3M í‰ê·  ëŒ€ë¹„ Â±100bp ----
if has_data(series_map.get("TSFR6M", pd.Series(dtype=float))):
    ts6_last = last_value(series_map["TSFR6M"])
    ts6_3m = trailing_3m_avg(series_map["TSFR6M"])
    dev_bp = (ts6_last - ts6_3m) * 100.0 if pd.notna(ts6_last) and pd.notna(ts6_3m) else np.nan
    rows.append({
        "metric": "TSFR 6M vs 3M Avg",
        "ticker": TICKERS["TSFR6M"],
        "latest": ts6_last,
        "breach_3m": (abs(dev_bp) >= THRESHOLDS["G3M_dev_bp"]) if pd.notna(dev_bp) else np.nan,
        "note": f"3M avg={ts6_3m:.4f}, dev={dev_bp:.1f}bp; ì„ê³„Â±{THRESHOLDS['G3M_dev_bp']}bp" if pd.notna(dev_bp) else "ë°ì´í„° ë¶€ì¡±",
    })

# ---- (E) USDKRW: 1ì¼ Â±2%, 10ì¼ Â±5% ----
if has_data(series_map.get("USDKRW", pd.Series(dtype=float))):
    krw_1d = pct_change(series_map["USDKRW"], 1)
    krw_10d = pct_change(series_map["USDKRW"], 10)
    rows.append({
        "metric": "USDKRW Spot",
        "ticker": TICKERS["USDKRW"],
        "latest": last_value(series_map["USDKRW"]),
        "chg_1d": f"{krw_1d:.2f}%" if pd.notna(krw_1d) else np.nan,
        "threshold_1d": "Â±2.0%",
        "breach_1d": (abs(krw_1d) >= THRESHOLDS["USFX_1d_pct"]) if pd.notna(krw_1d) else np.nan,
        "chg_10d": f"{krw_10d:.2f}%" if pd.notna(krw_10d) else np.nan,
        "threshold_10d": "Â±5.0%",
        "breach_10d": (abs(krw_10d) >= THRESHOLDS["USFX_10d_pct"]) if pd.notna(krw_10d) else np.nan,
        "note": "ì›/ë‹¬ëŸ¬ í™˜ìœ¨: % ê¸°ì¤€",
    })

# ---- (F) KOSPI: 1ì¼ -3.5%, 10ì¼ -10% (í•˜ë½ë§Œ) ----
if has_data(series_map.get("KOSPI", pd.Series(dtype=float))):
    k1 = pct_change(series_map["KOSPI"], 1)
    k10 = pct_change(series_map["KOSPI"], 10)
    rows.append({
        "metric": "KOSPI Index",
        "ticker": TICKERS["KOSPI"],
        "latest": last_value(series_map["KOSPI"]),
        "chg_1d": f"{k1:.2f}%" if pd.notna(k1) else np.nan,
        "threshold_1d": "â‰¤ -3.5%",
        "breach_1d": (k1 <= THRESHOLDS["KOSPI_1d_down_pct"]) if pd.notna(k1) else np.nan,
        "chg_10d": f"{k10:.2f}%" if pd.notna(k10) else np.nan,
        "threshold_10d": "â‰¤ -10.0%",
        "breach_10d": (k10 <= THRESHOLDS["KOSPI_10d_down_pct"]) if pd.notna(k10) else np.nan,
        "note": "í•˜ë½ ë°©í–¥ë§Œ íŠ¸ë¦¬ê±°",
    })

# ---- (G) VKOSPI: 1ì¼ +5pp, 10ì¼ +10pp (ìƒìŠ¹ë§Œ) ----
if has_data(series_map.get("VKOSPI", pd.Series(dtype=float))):
    v1 = pp_change(series_map["VKOSPI"], 1)
    v10 = pp_change(series_map["VKOSPI"], 10)
    rows.append({
        "metric": "VKOSPI (Vol Index)",
        "ticker": TICKERS["VKOSPI"],
        "latest": last_value(series_map["VKOSPI"]),
        "chg_1d": f"{v1:.2f}pp" if pd.notna(v1) else np.nan,
        "threshold_1d": "â‰¥ +5.0pp",
        "breach_1d": (v1 >= THRESHOLDS["VKOSPI_1d_up_pp"]) if pd.notna(v1) else np.nan,
        "chg_10d": f"{v10:.2f}pp" if pd.notna(v10) else np.nan,
        "threshold_10d": "â‰¥ +10.0pp",
        "breach_10d": (v10 >= THRESHOLDS["VKOSPI_10d_up_pp"]) if pd.notna(v10) else np.nan,
        "note": "ìƒìŠ¹ë§Œ íŠ¸ë¦¬ê±°(pp)",
    })

# ---- (H) USDKRW 1Y IV: 1ì¼ Â±5pp, 10ì¼ Â±10pp ----
if has_data(series_map.get("KRW_IV1Y", pd.Series(dtype=float))):
    iv1 = pp_change(series_map["KRW_IV1Y"], 1)
    iv10 = pp_change(series_map["KRW_IV1Y"], 10)
    rows.append({
        "metric": "USDKRW 1Y Implied Vol",
        "ticker": TICKERS["KRW_IV1Y"],
        "latest": last_value(series_map["KRW_IV1Y"]),
        "chg_1d": f"{iv1:.2f}pp" if pd.notna(iv1) else np.nan,
        "threshold_1d": "Â±5.0pp",
        "breach_1d": (abs(iv1) >= THRESHOLDS["KRWIV_1d_pp"]) if pd.notna(iv1) else np.nan,
        "chg_10d": f"{iv10:.2f}pp" if pd.notna(iv10) else np.nan,
        "threshold_10d": "Â±10.0pp",
        "breach_10d": (abs(iv10) >= THRESHOLDS["KRWIV_10d_pp"]) if pd.notna(iv10) else np.nan,
        "note": "ì ˆëŒ€ pp ê¸°ì¤€",
    })

# ---- (I) ì™¸í™” ì›”í‰ê·  ì¥ë‹¨ê¸°: (SOFR OIS 1Y - TSFR 1M) MTD â‰¥ +150bp ----
if has_data(series_map.get("SOFR_OIS_1Y", pd.Series(dtype=float))) and has_data(series_map.get("TSFR1M", pd.Series(dtype=float))):
    ois1y_mtd = month_avg(series_map["SOFR_OIS_1Y"], 0)
    tsfr1m_mtd = month_avg(series_map["TSFR1M"], 0)
    mtd_spread = (ois1y_mtd - tsfr1m_mtd) * 100.0 if pd.notna(ois1y_mtd) and pd.notna(tsfr1m_mtd) else np.nan
    rows.append({
        "metric": "USD OIS 1Y - TSFR 1M (MTD avg)",
        "ticker": f"{TICKERS['SOFR_OIS_1Y']} vs {TICKERS['TSFR1M']}",
        "chg_1d": f"{mtd_spread:.1f}bp (MTD spread)" if pd.notna(mtd_spread) else np.nan,
        "threshold_1d": f"â‰¥ +{THRESHOLDS['SPREAD_SOFR1M_vs_OIS1Y_MTD_bp']:.0f}bp",
        "breach_1d": (mtd_spread >= THRESHOLDS["SPREAD_SOFR1M_vs_OIS1Y_MTD_bp"]) if pd.notna(mtd_spread) else np.nan,
        "note": f"MTD OIS1Y={ois1y_mtd:.4f}, TSFR1M={tsfr1m_mtd:.4f}" if pd.notna(mtd_spread) else "ë°ì´í„°/í‹±ì»¤ í™•ì¸ í•„ìš”",
    })

# ---- (J) KR 1Y - ê¸°ì¤€ê¸ˆë¦¬: 5ì˜ì—…ì¼ ì—°ì† < -24bp ----
if has_data(series_map.get("KR1Y", pd.Series(dtype=float))) and has_data(series_map.get("KRBASERATE", pd.Series(dtype=float))):
    spr_bp = (series_map["KR1Y"] - series_map["KRBASERATE"]) * 100.0
    rows.append({
        "metric": "KR 1Y - BaseRate (level)",
        "ticker": f"{TICKERS['KR1Y']} - {TICKERS['KRBASERATE']}",
        "latest": float(spr_bp.dropna().iloc[-1]) if spr_bp.dropna().size else np.nan,
        "threshold_1d": "5ì˜ì—…ì¼ ì—°ì† < -24bp",
        "breach_1d": consec_last_n(spr_bp < THRESHOLDS["KR_1Y_minus_BASE_5d_level_bp"], 5) if spr_bp.dropna().size else np.nan,
        "note": "ë ˆë²¨ ê¸°ì¤€(ì¼ë³„ ìŠ¤í”„ë ˆë“œ)",
    })

# ---- (K) ê¸°ì¤€ê¸ˆë¦¬ - ì½œê¸ˆë¦¬: > +40bp ----
if has_data(series_map.get("KRBASERATE", pd.Series(dtype=float))) and has_data(series_map.get("KRCALL", pd.Series(dtype=float))):
    base_call = (series_map["KRBASERATE"] - series_map["KRCALL"]) * 100.0
    rows.append({
        "metric": "KR Base - Call (level)",
        "ticker": f"{TICKERS['KRBASERATE']} - {TICKERS['KRCALL']}",
        "latest": float(base_call.dropna().iloc[-1]) if base_call.dropna().size else np.nan,
        "threshold_1d": f"> +{THRESHOLDS['BASE_minus_CALL_bp']:.0f}bp",
        "breach_1d": (base_call.dropna().iloc[-1] > THRESHOLDS["BASE_minus_CALL_bp"]) if base_call.dropna().size else np.nan,
        "note": "ë ˆë²¨ ê¸°ì¤€",
    })

# ---- (L) TSFR 3M: MTD-PrevM ì ˆëŒ€ë³€í™” > 75bp ----
if has_data(series_map.get("TSFR3M", pd.Series(dtype=float))):
    ts3_cur = month_avg(series_map["TSFR3M"], 0)
    ts3_prev = month_avg(series_map["TSFR3M"], 1)
    ts3_diff = (ts3_cur - ts3_prev) * 100.0 if pd.notna(ts3_cur) and pd.notna(ts3_prev) else np.nan
    rows.append({
        "metric": "TSFR 3M (MTD - PrevM)",
        "ticker": TICKERS["TSFR3M"],
        "chg_1d": f"{ts3_diff:.1f}bp (Î”avg)" if pd.notna(ts3_diff) else np.nan,
        "threshold_1d": f"abs(Î”) > {THRESHOLDS['TSFR3M_prevM_abs_bp']:.0f}bp",
        "breach_1d": (abs(ts3_diff) > THRESHOLDS["TSFR3M_prevM_abs_bp"]) if pd.notna(ts3_diff) else np.nan,
        "note": f"MTD={ts3_cur:.4f}, PrevM={ts3_prev:.4f}" if pd.notna(ts3_diff) else "ë°ì´í„°/í‹±ì»¤ í™•ì¸ í•„ìš”",
    })

# ---- (M) JPY 3M TIBOR: MTD-PrevM ì ˆëŒ€ë³€í™” > 25bp ----
if has_data(series_map.get("JPY_TIBOR3M", pd.Series(dtype=float))):
    tib_cur = month_avg(series_map["JPY_TIBOR3M"], 0)
    tib_prev = month_avg(series_map["JPY_TIBOR3M"], 1)
    tib_diff = (tib_cur - tib_prev) * 100.0 if pd.notna(tib_cur) and pd.notna(tib_prev) else np.nan
    rows.append({
        "metric": "JPY TIBOR 3M (MTD - PrevM)",
        "ticker": TICKERS["JPY_TIBOR3M"],
        "chg_1d": f"{tib_diff:.1f}bp (Î”avg)" if pd.notna(tib_diff) else np.nan,
        "threshold_1d": f"abs(Î”) > {THRESHOLDS['JPY_TIBOR3M_prevM_abs_bp']:.0f}bp",
        "breach_1d": (abs(tib_diff) > THRESHOLDS["JPY_TIBOR3M_prevM_abs_bp"]) if pd.notna(tib_diff) else np.nan,
        "note": f"MTD={tib_cur:.4f}, PrevM={tib_prev:.4f}" if pd.notna(tib_diff) else "ë°ì´í„°/í‹±ì»¤ í™•ì¸ í•„ìš”",
    })

# ---- (N) í•œêµ­ 5Y CDS: PrevM +100bp 3D / M-3 +200bp 3D ----
if has_data(series_map.get("Korea", pd.Series(dtype=float))):
    cds_kr = series_map["Korea"]
    prevM_avg = month_avg(cds_kr, 1)
    m3_avg = month_avg(cds_kr, 3)

    if pd.notna(prevM_avg):
        dev_prev = cds_kr - prevM_avg
        rows.append({
            "metric": "KR 5Y CDS vs PrevM (3D consec)",
            "ticker": CDS_TICKERS["Korea"],
            "latest": last_value(cds_kr),
            "threshold_1d": f"> +{THRESHOLDS['KR5YCDS_prevM_bp_3d']:.0f}bp for 3D",
            "breach_1d": consec_last_n(dev_prev > THRESHOLDS["KR5YCDS_prevM_bp_3d"], 3),
            "note": f"PrevM avg={prevM_avg:.1f}bp",
        })

    if pd.notna(m3_avg):
        dev_m3 = cds_kr - m3_avg
        rows.append({
            "metric": "KR 5Y CDS vs M-3 (3D consec)",
            "ticker": CDS_TICKERS["Korea"],
            "latest": last_value(cds_kr),
            "threshold_1d": f"> +{THRESHOLDS['KR5YCDS_M3ago_bp_3d']:.0f}bp for 3D",
            "breach_1d": consec_last_n(dev_m3 > THRESHOLDS["KR5YCDS_M3ago_bp_3d"], 3),
            "note": f"M-3 avg={m3_avg:.1f}bp",
        })

# ---- (O) KR Term Spread (10Y-3Y): 5D ì—­ì „ ì§€ì† ----
if has_data(series_map.get("KR10Y", pd.Series(dtype=float))) and has_data(series_map.get("KR3Y", pd.Series(dtype=float))):
    term_spread = (series_map["KR10Y"] - series_map["KR3Y"]) * 100.0
    rows.append({
        "metric": "KR Term Spread 10Y-3Y (5D inversion)",
        "ticker": f"{TICKERS['KR10Y']} - {TICKERS['KR3Y']}",
        "latest": float(term_spread.dropna().iloc[-1]) if term_spread.dropna().size else np.nan,
        "threshold_1d": "< 0bp for 5D",
        "breach_1d": consec_last_n(term_spread <= 0.0, THRESHOLDS["KR_10Y_3Y_inversion_5d"]) if term_spread.dropna().size else np.nan,
        "note": "10Y-3Y â‰¤ 0bp ìƒíƒœ 5D ì—°ì†",
    })

# ---- (P) êµ­ê°€ë³„ CDS 17ê°œêµ­: ì „ì›” í‰ê·  ëŒ€ë¹„ +30% ìƒìŠ¹ ----
for country, bb in CDS_TICKERS.items():
    if country == "Korea":
        continue  # í•œêµ­ì€ ìœ„ì—ì„œ bp ê¸°ì¤€ 3D ì—°ì† ë¡œì§ ì ìš©
    s = series_map.get(country, pd.Series(dtype=float))
    if has_data(s):
        mtd, prev = month_avg(s, 0), month_avg(s, 1)
        pct_up = ((mtd / prev - 1.0) * 100.0) if (pd.notna(mtd) and pd.notna(prev) and prev != 0) else np.nan
        rows.append({
            "metric": f"CDS 5Y: {country} (MTD vs PrevM)",
            "ticker": bb,
            "latest": last_value(s),
            "chg_1d": f"{pct_up:.1f}%" if pd.notna(pct_up) else np.nan,
            "threshold_1d": f"> +{THRESHOLDS['CDS_prevM_pct_up']:.0f}%",
            "breach_1d": (pct_up > THRESHOLDS["CDS_prevM_pct_up"]) if pd.notna(pct_up) else np.nan,
            "note": f"MTD={mtd:.1f}, PrevM={prev:.1f}" if pd.notna(pct_up) else "ë°ì´í„°/í‹±ì»¤ í™•ì¸ í•„ìš”",
        })

# ---- (Q) (íšŒì‚¬ì±„/êµ­ê³ ) 3Y ë¹„ìœ¨: ì „ì›”í‰ê·  ëŒ€ë¹„ +16% ìƒìŠ¹ ----
if has_data(series_map.get("KR3Y", pd.Series(dtype=float))) and has_data(series_map.get("KR_CORP3Y_AA-", pd.Series(dtype=float))):
    ktb3y_mtd  = month_avg(series_map["KR3Y"], 0)
    corp3y_mtd = month_avg(series_map["KR_CORP3Y_AA-"], 0)
    ktb3y_prev  = month_avg(series_map["KR3Y"], 1)
    corp3y_prev = month_avg(series_map["KR_CORP3Y_AA-"], 1)

    ratio_cur  = (ktb3y_mtd / corp3y_mtd) if (pd.notna(ktb3y_mtd) and pd.notna(corp3y_mtd) and corp3y_mtd != 0) else np.nan
    ratio_prev = (ktb3y_prev / corp3y_prev) if (pd.notna(ktb3y_prev) and pd.notna(corp3y_prev) and corp3y_prev != 0) else np.nan
    ratio_pct  = ((ratio_cur / ratio_prev - 1.0) * 100.0) if (pd.notna(ratio_cur) and pd.notna(ratio_prev) and ratio_prev != 0) else np.nan

    rows.append({
        "metric": "KTB3Y / Corp(AA-) 3Y (MTD vs PrevM)",
        "ticker": "KR3Y / KR_CORP3Y_AA-",
        "chg_1d": f"{ratio_pct:.1f}%" if pd.notna(ratio_pct) else np.nan,
        "threshold_1d": f"> +{THRESHOLDS['CorpAAminus_KTB3Y_ratio_prevM_pct']:.0f}%",
        "breach_1d": (ratio_pct > THRESHOLDS["CorpAAminus_KTB3Y_ratio_prevM_pct"]) if pd.notna(ratio_pct) else np.nan,
        "note": f"MTD={ratio_cur:.4f}, PrevM={ratio_prev:.4f}" if pd.notna(ratio_pct) else "ë°ì´í„°/í‹±ì»¤ í™•ì¸ í•„ìš”",
    })

# ---- (R) ì›”í‰ê·  ì¥ë‹¨ê¸° (ê¸ˆìœµì±„1Y - CD3M): MTD ìŠ¤í”„ë ˆë“œ â‰¥ +70bp ----
if has_data(series_map.get("KR_FIN1Y_AAA", pd.Series(dtype=float))) and has_data(series_map.get("KR_CD3M", pd.Series(dtype=float))):
    fin1y_mtd = month_avg(series_map["KR_FIN1Y_AAA"], 0)
    cd3m_mtd  = month_avg(series_map["KR_CD3M"], 0)
    fin_cd_bp = (fin1y_mtd - cd3m_mtd) * 100.0 if pd.notna(fin1y_mtd) and pd.notna(cd3m_mtd) else np.nan
    rows.append({
        "metric": "(MTD) Fin 1Y - CD 3M",
        "ticker": f"{TICKERS['KR_FIN1Y_AAA']} - {TICKERS['KR_CD3M']}",
        "chg_1d": f"{fin_cd_bp:.1f}bp" if pd.notna(fin_cd_bp) else np.nan,
        "threshold_1d": f"â‰¥ +{THRESHOLDS['Fin1Y_minus_CD3M_MTD_bp']:.0f}bp",
        "breach_1d": (fin_cd_bp >= THRESHOLDS["Fin1Y_minus_CD3M_MTD_bp"]) if pd.notna(fin_cd_bp) else np.nan,
        "note": f"MTD Fin1Y={fin1y_mtd:.4f}, CD3M={cd3m_mtd:.4f}" if pd.notna(fin_cd_bp) else "ë°ì´í„°/í‹±ì»¤ í™•ì¸ í•„ìš”",
    })

# ---- (S) Fin1Y AAA - KTB1Y: 5ì˜ì—…ì¼ ì—°ì† â‰¥ +50bp ----
if has_data(series_map.get("KR_FIN1Y_AAA", pd.Series(dtype=float))) and has_data(series_map.get("KR1Y", pd.Series(dtype=float))):
    fin_minus_ktb1y = (series_map["KR_FIN1Y_AAA"] - series_map["KR1Y"]) * 100.0
    rows.append({
        "metric": "Fin 1Y(AAA) - KTB 1Y (5D consec â‰¥50bp)",
        "ticker": f"{TICKERS['KR_FIN1Y_AAA']} - {TICKERS['KR1Y']}",
        "latest": float(fin_minus_ktb1y.dropna().iloc[-1]) if fin_minus_ktb1y.dropna().size else np.nan,
        "threshold_1d": f"â‰¥ +{THRESHOLDS['Fin1YAAA_minus_KTB1Y_5d_bp']:.0f}bp for 5D",
        "breach_1d": consec_last_n(fin_minus_ktb1y >= THRESHOLDS["Fin1YAAA_minus_KTB1Y_5d_bp"], 5) if fin_minus_ktb1y.dropna().size else np.nan,
        "note": "ë ˆë²¨ ê¸°ì¤€(ì¼ë³„ ìŠ¤í”„ë ˆë“œ)",
    })

# ---- (T) S&P 500: 1ì¼ â‰¤ -3%, 10ì¼ â‰¤ -12% ----
if has_data(series_map.get("SPX", pd.Series(dtype=float))):
    spx_1d = pct_change(series_map["SPX"], 1)
    spx_10d = pct_change(series_map["SPX"], 10)
    rows.append({
        "metric": "S&P 500",
        "ticker": TICKERS["SPX"],
        "latest": last_value(series_map["SPX"]),
        "chg_1d": f"{spx_1d:.2f}%" if pd.notna(spx_1d) else np.nan,
        "threshold_1d": "â‰¤ -3.0%",
        "breach_1d": (spx_1d <= THRESHOLDS["SPX_1d_down_pct"]) if pd.notna(spx_1d) else np.nan,
        "chg_10d": f"{spx_10d:.2f}%" if pd.notna(spx_10d) else np.nan,
        "threshold_10d": "â‰¤ -12.0%",
        "breach_10d": (spx_10d <= THRESHOLDS["SPX_10d_down_pct"]) if pd.notna(spx_10d) else np.nan,
        "note": "í•˜ë½ë§Œ íŠ¸ë¦¬ê±°",
    })

# ---- (U) EuroStoxx50: 1ì¼ |Î”| â‰¥ 3%, 10ì¼ â‰¤ -12% ----
if has_data(series_map.get("SX5E", pd.Series(dtype=float))):
    sx_1d = pct_change(series_map["SX5E"], 1)
    sx_10d = pct_change(series_map["SX5E"], 10)
    rows.append({
        "metric": "EuroStoxx50",
        "ticker": TICKERS["SX5E"],
        "latest": last_value(series_map["SX5E"]),
        "chg_1d": f"{sx_1d:.2f}%" if pd.notna(sx_1d) else np.nan,
        "threshold_1d": f"abs â‰¥ {THRESHOLDS['SX5E_1d_abs_pct']:.1f}%",
        "breach_1d": (abs(sx_1d) >= THRESHOLDS["SX5E_1d_abs_pct"]) if pd.notna(sx_1d) else np.nan,
        "chg_10d": f"{sx_10d:.2f}%" if pd.notna(sx_10d) else np.nan,
        "threshold_10d": "â‰¤ -12.0%",
        "breach_10d": (sx_10d <= THRESHOLDS["SX5E_10d_down_pct"]) if pd.notna(sx_10d) else np.nan,
        "note": "1DëŠ” ì ˆëŒ€ê°’, 10DëŠ” í•˜ë½ë§Œ",
    })

# ---- (V) 3M FRA-OIS: PrevM ëŒ€ë¹„ +30bp ----
if has_data(series_map.get("US_FRAOIS_3M", pd.Series(dtype=float))):
    fraois_mtd  = month_avg(series_map["US_FRAOIS_3M"], 0)
    fraois_prev = month_avg(series_map["US_FRAOIS_3M"], 1)
    diff_bp = (fraois_mtd - fraois_prev) * 100.0 if pd.notna(fraois_mtd) and pd.notna(fraois_prev) else np.nan
    rows.append({
        "metric": "USD 3M FRA-OIS (MTD - PrevM)",
        "ticker": TICKERS["US_FRAOIS_3M"],
        "chg_1d": f"{diff_bp:.1f}bp (Î”avg)" if pd.notna(diff_bp) else np.nan,
        "threshold_1d": f"> +{THRESHOLDS['FRAOIS_prevM_bp']:.0f}bp",
        "breach_1d": (diff_bp > THRESHOLDS["FRAOIS_prevM_bp"]) if pd.notna(diff_bp) else np.nan,
        "note": f"MTD={fraois_mtd:.2f}, PrevM={fraois_prev:.2f}" if pd.notna(diff_bp) else "ë°ì´í„°/í‹±ì»¤ í™•ì¸ í•„ìš”",
    })

# -----------------------------
# 7) ì—‘ì…€ ì €ì¥ (alerts + raw_data)
# -----------------------------
alerts_df = pd.DataFrame(rows)

order_cols = [
    "metric","ticker","latest",
    "chg_1d","threshold_1d","breach_1d",
    "chg_10d","threshold_10d","breach_10d",
    "breach_3m","note",
]
for c in order_cols:
    if c not in alerts_df.columns:
        alerts_df[c] = np.nan

alerts_df = alerts_df[order_cols]

with pd.ExcelWriter(output_path, engine="openpyxl") as writer:
    alerts_df.to_excel(writer, sheet_name="alerts", index=False)
    raw_df = hist.copy()
    raw_df.index.name = "Date"
    raw_df.reset_index().to_excel(writer, sheet_name="raw_data", index=False)

print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")
print("â–¶ alerts ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 20í–‰):")
print(alerts_df.head(20))


#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ë¦¬ìŠ¤í¬ ì„ê³„ì¹˜ ì´ˆê³¼ ë‚´ì—­ í…”ë ˆê·¸ë¨ ì „ì†¡ ìŠ¤í¬ë¦½íŠ¸

ëª©ì :
- ê¸°ì¡´ 'risk_thresholds_YYYYMMDD.xlsx' (alerts ì‹œíŠ¸)ë¥¼ ì½ì–´ì„œ
- breach_1d / breach_10d / breach_3m ì¤‘ í•˜ë‚˜ë¼ë„ Trueì¸ í•­ëª©ë§Œ ê³¨ë¼
- í…”ë ˆê·¸ë¨ìœ¼ë¡œ ìš”ì•½ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•˜ê³ , ë§ˆì§€ë§‰ ì¤„ì— ëŒ€ì‹œë³´ë“œ ë§í¬ë¥¼ ì¶”ê°€í•œë‹¤.

ì „ì œ:
- ì•ì„œ íŒ€ì¥ë‹˜ì´ ì‹¤í–‰í•˜ì‹  Bloomberg/xbbg ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•œ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ
  risk_thresholds_YYYYMMDD.xlsx íŒŒì¼ì´ ìƒì„±ë˜ì–´ ìˆì–´ì•¼ í•œë‹¤.
- í…”ë ˆê·¸ë¨ ë´‡ í† í°ê³¼ ì±„ë„/ì±„íŒ… ID ê°€ ì¤€ë¹„ë˜ì–´ ìˆì–´ì•¼ í•œë‹¤.

í•„ìš” íŒ¨í‚¤ì§€:
    pip install pandas requests openpyxl
"""

from pathlib import Path
from datetime import date
import pandas as pd
import numpy as np
import requests

# ==========================
# 1. í…”ë ˆê·¸ë¨ ì„¤ì •ê°’
# ==========================
BOT_TOKEN = "8432426313:AAEdvkoFEozZE-1F0ARc82i_JXCWA4fPfgE"  # íŒ€ì¥ë‹˜ ë´‡ í† í°
CHAT_ID   = "-4956067497"                                     # íŒ€ì¥ë‹˜ ì±„ë„/ì±„íŒ… ID

TELEGRAM_API_URL = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"

# ==========================
# 2. íŒŒì¼ ê²½ë¡œ ì„¤ì •
# ==========================
# - risk_thresholds_YYYYMMDD.xlsx ê°€ ì €ì¥ëœ í´ë”
OUTPUT_DIR = Path(r"C:/Users/amongpapa/chartup/raw_data")

# - ê¸°ë³¸ì€ "ì˜¤ëŠ˜ ë‚ ì§œ" íŒŒì¼ì„ ì°¾ë„ë¡ êµ¬ì„±
#   (í•„ìš”í•˜ë©´ mainì—ì„œ execution_dateë¥¼ ì¸ìë¡œ ë°›ë„ë¡ í™•ì¥í•´ë„ ë¨)
def get_risk_excel_path(target_date: date | None = None) -> Path:
    """
    ëŒ€ìƒ ë‚ ì§œì˜ risk_thresholds_YYYYMMDD.xlsx ê²½ë¡œë¥¼ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜.
    target_dateë¥¼ ë„˜ê¸°ì§€ ì•Šìœ¼ë©´ ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ íŒŒì¼ëª…ì„ ë§Œë“ ë‹¤.
    """
    if target_date is None:
        target_date = date.today()
    fname = f"risk_thresholds_{target_date:%Y%m%d}.xlsx"
    return OUTPUT_DIR / fname


# ==========================
# 3. ì—‘ì…€ì—ì„œ breach í•­ëª© ì¶”ì¶œ
# ==========================
def load_breach_rows(excel_path: Path) -> pd.DataFrame:
    """
    risk_thresholds ì—‘ì…€ íŒŒì¼ì—ì„œ alerts ì‹œíŠ¸ë¥¼ ì½ê³ ,
    breach_1d / breach_10d / breach_3m ì¤‘ í•˜ë‚˜ë¼ë„ Trueì¸ í–‰ë§Œ í•„í„°ë§í•´ì„œ ë°˜í™˜.

    ë°˜í™˜:
        breach_df (DataFrame): breach í–‰ë§Œ ëª¨ì€ ìš”ì•½ í…Œì´ë¸”
    """
    if not excel_path.exists():
        raise FileNotFoundError(f"ì„ê³„ì¹˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {excel_path}")

    # alerts ì‹œíŠ¸ ì½ê¸°
    df = pd.read_excel(excel_path, sheet_name="alerts")

    # breach ì»¬ëŸ¼ë“¤ë§Œ ìë™ íƒìƒ‰ (breachë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ì»¬ëŸ¼)
    breach_cols = [c for c in df.columns if c.startswith("breach")]
    if not breach_cols:
        # breach ì»¬ëŸ¼ì´ ì•„ì˜ˆ ì—†ìœ¼ë©´ ë¹ˆ DF ë°˜í™˜
        return df.iloc[0:0].copy()

    # NaN â†’ False ë¡œ ì±„ìš°ê³  í–‰ ë‹¨ìœ„ë¡œ OR ì¡°ê±´
    mask = df[breach_cols].fillna(False).any(axis=1)
    breach_df = df.loc[mask].copy()

    return breach_df


# ==========================
# 4. í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ í…ìŠ¤íŠ¸ êµ¬ì„±
# ==========================
def format_value(x) -> str:
    """
    NaN, None ì€ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” ë‹¨ìˆœ í—¬í¼.
    """
    if pd.isna(x):
        return ""
    return str(x)


def build_message_from_breach_df(breach_df: pd.DataFrame, target_date: date) -> str:
    """
    breach_dfë¥¼ ê¸°ë°˜ìœ¼ë¡œ í…”ë ˆê·¸ë¨ì— ë³´ë‚¼ ë©”ì‹œì§€ ë³¸ë¬¸ì„ êµ¬ì„±í•œë‹¤.

    ê·œì¹™:
    - í—¤ë”: ë‚ ì§œ + ì œëª©
    - ê° í•­ëª©:
        * metric (ì§€í‘œ ì´ë¦„)
        * (í•„ìš” ì‹œ) latest ê°’
        * breach_1d / breach_10d / breach_3m ì¤‘ Trueì¸ ê²ƒë§Œ ê³¨ë¼ì„œ
          ë³€í™”ëŸ‰ + ê¸°ì¤€ì„ í•œ ì¤„ì”© bullet ë¡œ í‘œì‹œ
        * noteê°€ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ì— ê´„í˜¸ë¡œ ì¶”ê°€
    - ë§ˆì§€ë§‰ ì¤„ì— risk_monitor ë§í¬ ì¶”ê°€
    """
    header_lines = [
        f"[ë¦¬ìŠ¤í¬ ì„ê³„ì¹˜ ì´ˆê³¼ ì•Œë¦¼]",
        f"{target_date:%Y-%m-%d} ê¸°ì¤€",
        "",
    ]

    body_lines: list[str] = []

    if breach_df.empty:
        # ì„ê³„ì¹˜ ì´ˆê³¼ í•­ëª©ì´ ì—†ì„ ë•Œ
        body_lines.append("ì˜¤ëŠ˜ì€ ì„¤ì •ëœ ì„ê³„ìˆ˜ì¤€ì„ ì´ˆê³¼í•œ ì§€í‘œê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # ê° í–‰ë³„ë¡œ í…ìŠ¤íŠ¸ ì •ë¦¬
        for idx, row in breach_df.iterrows():
            metric = format_value(row.get("metric", ""))
            ticker = format_value(row.get("ticker", ""))
            latest = row.get("latest", np.nan)
            latest_str = format_value(latest)

            line_header = f"â€¢ {metric}"
            if ticker:
                line_header += f" ({ticker})"
            body_lines.append(line_header)

            # ì–´ë–¤ breachê°€ ë°œìƒí–ˆëŠ”ì§€ ì •ë¦¬
            # 1ì¼
            if bool(row.get("breach_1d", False)):
                chg_1d = format_value(row.get("chg_1d", ""))
                th_1d  = format_value(row.get("threshold_1d", ""))
                body_lines.append(f"   - 1ì¼ ë³€í™”: {chg_1d} (ê¸°ì¤€ {th_1d})")

            # 10ì¼
            if bool(row.get("breach_10d", False)):
                chg_10d = format_value(row.get("chg_10d", ""))
                th_10d  = format_value(row.get("threshold_10d", ""))
                body_lines.append(f"   - 10ì¼ ë³€í™”: {chg_10d} (ê¸°ì¤€ {th_10d})")

            # 3ê°œì›”/ê¸°íƒ€ ê¸°ì¤€ (breach_3m ì»¬ëŸ¼)
            if bool(row.get("breach_3m", False)):
                note = format_value(row.get("note", ""))
                if note:
                    body_lines.append(f"   - 3ê°œì›”/í‰ê·  ê¸°ì¤€ ì´ˆê³¼: {note}")
                else:
                    body_lines.append(f"   - 3ê°œì›”/í‰ê·  ê¸°ì¤€ ì´ˆê³¼")

            # latest ê°’ì´ ì˜ë¯¸ ìˆì„ ê²½ìš° í•œ ì¤„ ì¶”ê°€ (ì›ì¹˜ ì•Šìœ¼ë©´ ì£¼ì„ ì²˜ë¦¬ ê°€ëŠ¥)
            if latest_str:
                body_lines.append(f"   - í˜„ì¬ ìˆ˜ì¤€: {latest_str}")

            # í•­ëª© ê°„ êµ¬ë¶„ìš© ë¹ˆ ì¤„
            body_lines.append("")

    # footer: ë§í¬ ì¶”ê°€
    footer_lines = [
        "",
        "ìì„¸í•œ ì§€í‘œëŠ” ë‚´ë¶€ ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸í•´ì£¼ì„¸ìš”.",
        "https://chartupndown.com/risk_monitor",
    ]

    # ì „ì²´ ë©”ì‹œì§€ í•©ì¹˜ê¸°
    full_message = "\n".join(header_lines + body_lines + footer_lines)
    return full_message


# ==========================
# 5. í…”ë ˆê·¸ë¨ ì „ì†¡ í•¨ìˆ˜
# ==========================
def send_telegram_message(text: str) -> None:
    """
    í…”ë ˆê·¸ë¨ sendMessage APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì§€ì •ëœ CHAT_IDë¡œ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•œë‹¤.
    """
    payload = {
        "chat_id": CHAT_ID,
        "text": text,
        "parse_mode": "HTML",  # í•„ìš”ì‹œ Markdown ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥
    }
    try:
        resp = requests.post(TELEGRAM_API_URL, data=payload, timeout=10)
        resp.raise_for_status()
        print("âœ… í…”ë ˆê·¸ë¨ ì „ì†¡ ì„±ê³µ")
    except requests.RequestException as e:
        print("âš ï¸ í…”ë ˆê·¸ë¨ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:", e)


# ==========================
# 6. ë©”ì¸ ì‹¤í–‰ íë¦„
# ==========================
def send_risk_alert_via_telegram(target_date: date | None = None) -> None:
    """
    í†µí•© ì‹¤í–‰ í•¨ìˆ˜:
    1) ëŒ€ìƒ ë‚ ì§œì˜ risk_thresholds ì—‘ì…€ ê²½ë¡œ ê³„ì‚°
    2) breach í–‰ë§Œ í•„í„°ë§
    3) í…ìŠ¤íŠ¸ ë©”ì‹œì§€ êµ¬ì„±
    4) í…”ë ˆê·¸ë¨ ì „ì†¡
    """
    if target_date is None:
        target_date = date.today()

    excel_path = get_risk_excel_path(target_date)
    print(f"â–¶ ì„ê³„ì¹˜ íŒŒì¼: {excel_path}")

    breach_df = load_breach_rows(excel_path)
    print(f"â–¶ ì„ê³„ì¹˜ ì´ˆê³¼ ì§€í‘œ ìˆ˜: {len(breach_df)}")

    msg = build_message_from_breach_df(breach_df, target_date)
    print("â–¶ ì „ì†¡ ë©”ì‹œì§€ ë¯¸ë¦¬ë³´ê¸°:")
    print("=" * 60)
    print(msg)
    print("=" * 60)

    # ì‹¤ì œ í…”ë ˆê·¸ë¨ ì „ì†¡
    send_telegram_message(msg)


# ==========================
# 7. ì§ì ‘ ì‹¤í–‰ ì‹œ ì§„ì…ì 
# ==========================
if __name__ == "__main__":
    # ê¸°ë³¸ì€ ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ë™ì‘
    send_risk_alert_via_telegram()

    # íŠ¹ì • ë‚ ì§œ íŒŒì¼ì„ ë³´ë‚´ê³  ì‹¶ìœ¼ë©´ ì˜ˆì‹œì²˜ëŸ¼ í˜¸ì¶œ
    # from datetime import datetime
    # send_risk_alert_via_telegram(datetime(2025, 11, 17).date())


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â–¶ í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜ (í•œ ë²ˆë§Œ ì‹¤í–‰)
# pip install xbbg blpapi pandas openpyxl
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import os
from datetime import datetime, timedelta
import pandas as pd
import blpapi                                   # Bloomberg lowâ€‘level API
from xbbg import blp                            # xbbg ë˜í¼

# 1) í™˜ê²½ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì…ë ¥ íŒŒì¼ ê²½ë¡œ: indicator.xlsx
input_path = r"C:\Users\amongpapa\chartup\go_scen\data\indicator.xlsx"

# ì¶œë ¥ì„ ì €ì¥í•  í´ë” (set)
output_dir = r"C:\Users\amongpapa\chartup\go_scen\data\set"

# ì¡°íšŒ ê¸°ê°„: ì˜¤ëŠ˜ ê¸°ì¤€ ê³¼ê±° 1ë…„
today      = datetime.today()
end_date   = today.strftime("%Y-%m-%d")                        # ì˜ˆ: '2025-07-29'
start_date = (today - timedelta(days=365)).strftime("%Y-%m-%d")  # ì˜ˆ: '2024-07-29'

# 2) ì¶œë ¥ í´ë” ì¤€ë¹„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.makedirs(output_dir, exist_ok=True)  # ì—†ìœ¼ë©´ ìƒì„±

# 3) indicator.xlsx ì½ê¸°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df_ind = pd.read_excel(input_path, dtype=str)  # ëª¨ë“  ì¹¼ëŸ¼ì„ ë¬¸ìì—´ë¡œ ë¡œë“œ

# í•„ìˆ˜ ì¹¼ëŸ¼ í™•ì¸
required_cols = {'Indicator_ID', 'Bloomberg_Ticker'}
if not required_cols.issubset(df_ind.columns):
    raise KeyError(f"'{', '.join(required_cols)}' ì¹¼ëŸ¼ì´ ëª¨ë‘ í•„ìš”í•©ë‹ˆë‹¤. íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

# 4) í‹°ì»¤ë³„ ì¡°íšŒ ë° ì €ì¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
total = len(df_ind)
for idx, row in df_ind.iterrows():
    indicator_id = row['Indicator_ID'].strip()      # ì§€í‘œ ê³ ìœ  ID
    ticker       = row['Bloomberg_Ticker'].strip()   # ë¸”ë£¸ë²„ê·¸ í‹°ì»¤

    print(f"[{idx+1}/{total}] ì¡°íšŒ ì‹œì‘ â–¶ {indicator_id} ({ticker})")

    try:
        # PX_LAST(ì¢…ê°€) ì¼ë³„ ì‹œê³„ì—´ ì¡°íšŒ
        df_ts = blp.bdh(
            tickers=[ticker],        # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì…ë ¥
            flds='PX_LAST',          # ì¡°íšŒ í•„ë“œ: ì¢…ê°€
            start_date=start_date,   # ì‹œì‘ì¼
            end_date=end_date,       # ì¢…ë£Œì¼
            Per='D',                 # ì¼ë³„ ë°ì´í„°
            adjust='all'             # ë°°ë‹¹Â·ë¶„í•  ë“± ì¡°ì • ë°˜ì˜
        )
    except Exception as e:
        print(f"âš ï¸ ì¡°íšŒ ì‹¤íŒ¨: {indicator_id} ({ticker}) â†’ {e}")
        continue

    if df_ts.empty:
        print(f"âš ï¸ ë°ì´í„° ì—†ìŒ: {indicator_id} ({ticker}) ëŠ” ìŠ¤í‚µí•©ë‹ˆë‹¤.")
        continue

    # ì—‘ì…€ë¡œ ì €ì¥
    output_path = os.path.join(output_dir, f"{indicator_id}.xlsx")
    df_ts.to_excel(output_path, engine='openpyxl')
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")

print("ğŸ‰ ëª¨ë“  ì§€í‘œ ì¡°íšŒ ë° ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤.")


# -*- coding: utf-8 -*-
"""
IND001 ~ IND151 ì—‘ì…€ íŒŒì¼ì— ëŒ€í•´:
1) ì‹œê³„ì—´(ë‚ ì§œ, ê°’) ìë™ íƒì§€
2) ìµœê·¼ í‰ê· /í‘œì¤€í¸ì°¨ ê¸°ë°˜ ì„ê³„ì¹˜ ê³„ì‚°
3) ë§ˆì§€ë§‰ ê°’ì´ ì„ê³„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ
      - ì •ìƒ       â†’ C1 = 'G'
      - ì£¼ì˜/ê²½ê³ /ì‹¬ê° â†’ C1 = 'Y'  (ì£¼ì˜ ì´ìƒì€ ëª¨ë‘ Y)
4) D1ì— 60ì¼ í‰ê· (mu), E1ì— 60ì¼ í‘œì¤€í¸ì°¨(sd=Ïƒ) ê¸°ë¡
5) ì‹¤í–‰ ìš”ì•½ CSV ì €ì¥

ì„¤ì¹˜:
    pip install pandas openpyxl numpy

ì‚¬ìš©:
    python script.py
"""

import os
import re
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Optional, Tuple

# ================= íŒ€ì¥ë‹˜ í™˜ê²½ ì„¤ì • =================
BASE_DIR = r"C:\Users\amongpapa\chartup\go_scen\data\set"  # ì—‘ì…€ í´ë” ê²½ë¡œ (raw string ì‚¬ìš©)
FILE_PREFIX = "IND"                                       # íŒŒì¼ ì ‘ë‘ì‚¬
FILE_RANGE = range(1, 152)                                # 001~151

SHEET_NAME = 0                  # ì²« ë²ˆì§¸ ì‹œíŠ¸(ì •ìˆ˜ ë˜ëŠ” ì‹œíŠ¸ëª…)
WINDOW = 60                     # ë¡¤ë§ ìœˆë„ìš°(ì¼ìˆ˜, í‰ê· /í‘œì¤€í¸ì°¨ ê³„ì‚°ìš©)
K = 2.0                         # (í˜„ì¬ëŠ” z-score ì°¸ê³ ìš©, ì„ê³„ì¹˜ì—ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
MIN_PERIODS = max(30, WINDOW // 2)  # ìµœì†Œ ê³„ì‚° ë°ì´í„° ê¸¸ì´(ì§§ì€ ë°ì´í„° ë³´í˜¸)

# ì»¬ëŸ¼ ì´ë¦„ íŒíŠ¸(ìš°ì„  ë§¤ì¹­)
DATE_COL_HINTS = ["date", "ë‚ ì§œ", "ì¼ì", "time", "ì¼ì‹œ"]
VALUE_COL_HINTS = ["close", "price", "value", "index", "ì§€ìˆ˜", "ì¢…ê°€", "ê°€ê²©", "ê°’", "ìˆ˜ì¹˜", "PX_LAST"]
# ====================================================

# âœ… [ì¶”ê°€] ì§€í‘œë³„ ê³ ì • ì„ê³„ì¹˜ ì„¤ì • (ë°©ì‹ A: ì ˆëŒ€ê°’ ê¸°ì¤€)
# - ì—¬ê¸° ìˆëŠ” IDëŠ” ê³ ì • ì„ê³„ì¹˜ë¥¼ ì‚¬ìš©
# - ì—¬ê¸°ì— ì—†ëŠ” IDëŠ” ìë™ ê³„ì‚° ì„ê³„ì¹˜(í‰ê·  Ã— 1.1 / 1.2 / 1.3)ë¥¼ ì‚¬ìš©
CUSTOM_THRESHOLDS = {
    # USD/KRW í™˜ìœ¨ (IND071): ê°’ì´ ì»¤ì§ˆìˆ˜ë¡ ë¦¬ìŠ¤í¬â†‘
    "IND071": {
        "direction": "up",   # up: ê°’ì´ ì»¤ì§ˆìˆ˜ë¡ ìœ„í—˜, down: ê°’ì´ ì‘ì•„ì§ˆìˆ˜ë¡ ìœ„í—˜
        "yellow": 1400.0,    # ì£¼ì˜
        "orange": 1600.0,    # ê²½ê³ 
        "red": 1800.0        # ì‹¬ê°
    },
    # í•„ìš”í•˜ë©´ ì½”ìŠ¤í”¼/ë¯¸êµ­ ê¸ˆë¦¬ ë“± ì¶”ê°€ ê°€ëŠ¥
    # "IND001": {
    #     "direction": "down",
    #     "yellow": 2400.0,
    #     "orange": 2200.0,
    #     "red": 2000.0
    # },
    # "IND050": {
    #     "direction": "up",
    #     "yellow": 4.5,
    #     "orange": 5.0,
    #     "red": 5.5
    # },
}


# ---------------- ë„ìš°ë¯¸: ì¤‘ë³µ ì»¬ëŸ¼ëª… ìœ ì¼í™” ----------------
def _make_unique(names):
    """
    ë™ì¼í•œ ì»¬ëŸ¼ëª…ì´ ë°˜ë³µë  ê²½ìš° .1, .2 ì ‘ë¯¸ì‚¬ë¥¼ ë¶€ì—¬í•´ ìœ ì¼í™”
    ì˜ˆ) ['nan','nan'] -> ['nan','nan.1']
    """
    seen = {}
    out = []
    for n in names:
        key = str(n)
        if key not in seen:
            seen[key] = 0
            out.append(key)
        else:
            seen[key] += 1
            out.append(f"{key}.{seen[key]}")
    return out


def _clean_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    ê³µë°± ì •ë¦¬ + ìœ ì¼í™”
    """
    cols = [re.sub(r"\s+", " ", str(c)).strip() for c in df.columns]
    df.columns = _make_unique(cols)
    return df


# ---------------- ìœ í‹¸: ìˆ«ì ë¬¸ìì—´ â†’ float (í•­ìƒ Series ë°˜í™˜) ----------------
def _as_numeric_series(s) -> pd.Series:
    """
    ì–´ë–¤ ì…ë ¥(s: Series/DataFrame/ndarray/ë¦¬ìŠ¤íŠ¸)ì´ ì™€ë„ **í•­ìƒ pandas.Series**ë¡œ ë³€í™˜ í›„
    ì•ˆì „í•˜ê²Œ ìˆ«ìë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    - ê³µë°±/ëŒ€ì‹œë¥˜("", "-", "â€”", "_") â†’ NaN
    - ê´„í˜¸ ìŒìˆ˜ "(123)" â†’ -123
    - ì½¤ë§ˆ ì œê±° "1,234.56" â†’ 1234.56
    - í¼ì„¼íŠ¸ "5.2%" â†’ 0.052
    """
    if isinstance(s, pd.DataFrame):
        # ìˆ«ì ë³€í™˜ë¥ ì´ ê°€ì¥ ë†’ì€ ì»¬ëŸ¼ í•˜ë‚˜ ìë™ ì„ íƒ
        best_col = None
        best_ratio = -1.0
        for col in s.columns:
            x = pd.to_numeric(
                pd.Series(s[col]).astype("string")
                .str.replace(",", "", regex=False)
                .str.replace("%", "", regex=False),
                errors="coerce",
            )
            ratio = x.notna().mean()
            if ratio > best_ratio:
                best_ratio, best_col = ratio, col
        s = s[best_col]

    if not isinstance(s, pd.Series):
        s = pd.Series(s)

    ss = s.astype("string").str.strip()
    ss = ss.replace(
        {
            "": pd.NA,
            "-": pd.NA,
            "â€”": pd.NA,
            "_": pd.NA,
            "nan": pd.NA,
            "NaN": pd.NA,
            "None": pd.NA,
        }
    )

    # ê´„í˜¸ ìŒìˆ˜ ì²˜ë¦¬
    neg_mask = ss.str.match(r"^\(.*\)$", na=False)
    ss2 = ss.str.replace(r"^\((.*)\)$", r"\1", regex=True)  # '(123)' -> '123'

    # ì½¤ë§ˆ/í¼ì„¼íŠ¸ ì œê±°
    ss2 = ss2.str.replace(",", "", regex=False)
    pct_mask = ss2.str.endswith("%", na=False)
    ss2 = ss2.str.replace("%", "", regex=False)

    # ìˆ«ì ë³€í™˜
    num = pd.to_numeric(ss2, errors="coerce")

    # ê´„í˜¸ ìŒìˆ˜ ì ìš©
    if neg_mask.any():
        num.loc[neg_mask & num.notna()] = -num.loc[neg_mask & num.notna()].abs()

    # í¼ì„¼íŠ¸ â†’ /100
    if pct_mask.any():
        num.loc[pct_mask & num.notna()] = num.loc[pct_mask & num.notna()] / 100.0

    return num.astype("float64")


# ---------------- ìŠ¤ì½”ì–´ë§ í—¬í¼ ----------------
def _numeric_score(df: pd.DataFrame, lookahead: int = 5) -> int:
    """
    ìƒìœ„ lookaheadí–‰ì—ì„œ ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•œ ì—´ ê°œìˆ˜ ìŠ¤ì½”ì–´ë§.
    """
    head = df.head(lookahead)
    cnt = 0
    for c in head.columns:
        s = head[c]
        s_num = _as_numeric_series(s)
        numlike = s_num.notna().mean()
        if numlike >= 0.5:
            cnt += 1
    return cnt


def _likely_good(df: pd.DataFrame) -> bool:
    """í˜„ì¬ í˜•íƒœê°€ 'í—¤ë”-ë°ì´í„°'ë¡œ ì ì ˆí•œì§€ ê°„ë‹¨ ìŠ¤ì½”ì–´."""
    return _numeric_score(df, lookahead=5) >= 2  # ìˆ«ìí˜• ì—´ 2ê°œ ì´ìƒì´ë©´ OK


# ---------------- í—¤ë” ìë™ ê°ì§€ ----------------
def detect_header_and_read(path, sheet_name=0, max_scan=10) -> pd.DataFrame:
    """
    í—¤ë”/ë°ì´í„° ì‹œì‘í–‰ì´ ë¶ˆëª…í™•í•œ ì—‘ì…€ì„ ìœ„í•œ ë¡œë”.
    1) ì¼ë°˜ ë¡œë“œ í›„ ê°„ì´ ì ê²€, ê´œì°®ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì»¬ëŸ¼ ìœ ì¼í™” ì ìš©)
    2) ìƒë‹¨ max_scaní–‰ì„ í›„ë³´ í—¤ë”ë¡œ ê°€ì •í•´ ìŠ¤ì½”ì–´ë§ í›„ ë² ìŠ¤íŠ¸ í—¤ë” ì±„íƒ
    """
    # 1) ì¼ë°˜ ë¡œë“œ ì‹œë„
    df0 = pd.read_excel(path, sheet_name=sheet_name, engine="openpyxl", header=0)
    if df0.shape[0] == 0:
        df0 = pd.read_excel(path, sheet_name=sheet_name, engine="openpyxl", header=None)
    if _likely_good(df0):
        return _clean_columns(df0)

    # 2) í›„ë³´ í—¤ë” ìŠ¤ìº”
    raw = pd.read_excel(path, sheet_name=sheet_name, engine="openpyxl", header=None)
    nrows = min(max_scan, len(raw))
    best_score, best_row = -1e9, 0
    for hdr in range(nrows):
        tmp = raw.copy()
        tmp.columns = _make_unique(tmp.iloc[hdr].astype(str).str.strip())
        tmp = tmp.iloc[hdr + 1:].reset_index(drop=True)

        score_num = _numeric_score(tmp, lookahead=5)
        penalty = sum(
            1
            for name in tmp.columns
            if str(name).lower() in ("nan", "nat", "", "none")
        )
        score = score_num - 0.5 * penalty  # ë‚˜ìœ í—¤ë” íŒ¨ë„í‹°

        if score > best_score:
            best_score, best_row = score, hdr

    raw.columns = _make_unique(raw.iloc[best_row].astype(str).str.strip())
    df = raw.iloc[best_row + 1:].reset_index(drop=True)
    return _clean_columns(df)


# ---------------- ë‚ ì§œ/ê°’ ì»¬ëŸ¼ ìë™ ì„ íƒ ----------------
def _pick_date_col(df: pd.DataFrame) -> Optional[str]:
    # íŒíŠ¸ ìš°ì„ 
    for hint in DATE_COL_HINTS:
        for c in df.columns:
            if hint.lower() in str(c).lower():
                return c
    # dtype/ë³€í™˜ë¥  ê¸°ë°˜
    dt_cols = [c for c in df.columns if pd.api.types.is_datetime64_any_dtype(df[c])]
    if dt_cols:
        return dt_cols[0]
    best_col, best_ratio = None, 0.0
    for c in df.columns:
        try:
            converted = pd.to_datetime(df[c], errors="coerce", infer_datetime_format=True)
            ratio = converted.notna().mean()
            if ratio > best_ratio and ratio >= 0.7:
                best_col, best_ratio = c, ratio
        except Exception:
            pass
    return best_col


def _pick_value_col(df: pd.DataFrame, exclude_cols: list) -> Optional[str]:
    """
    ê°’ ì»¬ëŸ¼ ìë™ ì„ íƒ(íŒíŠ¸ â†’ í†µê³„ ìŠ¤ì½”ì–´).
    """
    # íŒíŠ¸ ìš°ì„ 
    for hint in VALUE_COL_HINTS:
        for c in df.columns:
            if c in exclude_cols:
                continue
            if hint.lower() in str(c).lower():
                return c

    # ìˆ«ìí˜• í›„ë³´ ìŠ¤ì½”ì–´ë§(ê²°ì¸¡â†“, ë¶„ì‚°>0, ìƒ˜í”Œ ìˆ˜ ì¶©ë¶„)
    candidates = []
    for c in df.columns:
        if c in exclude_cols:
            continue
        series = _as_numeric_series(df[c])
        na_ratio = pd.isna(series).mean()
        var = np.nanvar(series.astype(float))
        if (~pd.isna(series)).sum() >= MIN_PERIODS and var > 0:
            candidates.append((c, na_ratio, var))

    if not candidates:
        # MIN_PERIODS ë¯¸ë§Œì´ì–´ë„ ìµœì„ ì˜ ìˆ«ìí˜• ì»¬ëŸ¼ì„ fallback
        fallback = []
        for c in df.columns:
            if c in exclude_cols:
                continue
            series = _as_numeric_series(df[c])
            var = np.nanvar(series.astype(float))
            if var > 0:
                na_ratio = pd.isna(series).mean()
                fallback.append((c, na_ratio, var))
        if not fallback:
            return None
        fallback.sort(key=lambda x: (x[1], -x[2]))
        return fallback[0][0]

    candidates.sort(key=lambda x: (x[1], -x[2]))
    return candidates[0][0]


# ---------------- ë¡œë”©/ê³„ì‚° ----------------
def _load_timeseries(path: str, sheet_name=0) -> Tuple[pd.Series, pd.DataFrame]:
    """
    ì—‘ì…€ì—ì„œ ì‹œê³„ì—´(ë‚ ì§œ, ê°’)ì„ ì¶”ì¶œí•´ Series ë°˜í™˜.
    - ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ DatetimeIndex ì •ë ¬
    - ì—†ìœ¼ë©´ ë‹¨ìˆœ ìˆœë²ˆ ì¸ë±ìŠ¤
    """
    df = detect_header_and_read(path, sheet_name=sheet_name)

    date_col = _pick_date_col(df)
    value_col = _pick_value_col(df, exclude_cols=[date_col] if date_col else [])

    if value_col is None:
        raise ValueError("ìˆ«ìí˜• ì‹œê³„ì—´ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (í—¤ë”/í˜•ì‹ í™•ì¸ í•„ìš”)")

    if date_col is not None:
        dt = pd.to_datetime(df[date_col], errors="coerce", infer_datetime_format=True)
        df = df.loc[dt.notna()].copy()
        df.index = pd.to_datetime(df[date_col], errors="coerce")
        df = df.sort_index()
    else:
        df = df.reset_index(drop=True)

    s = _as_numeric_series(df[value_col]).astype(float).dropna()

    # ê°™ì€ ë‚ ì§œ ì¤‘ë³µ â†’ í‰ê· 
    if isinstance(df.index, pd.DatetimeIndex) and s.index.has_duplicates:
        s = s.groupby(level=0).mean()

    return s.sort_index(), df


# ---------------- ì„ê³„ì¹˜ ê¸°ë°˜ í”Œë˜ê·¸ ê³„ì‚° ----------------
def compute_threshold_flag(
    s: pd.Series,
    indicator_id: Optional[str] = None,
    window: int = 60,
    k: float = 2.0,
) -> Tuple[str, dict]:
    """
    1) 60ì¼ ë¡¤ë§ í‰ê· (mu), í‘œì¤€í¸ì°¨(sd) ê³„ì‚°
    2) ì„ê³„ì¹˜ ê²°ì •
       - CUSTOM_THRESHOLDSì— ë“±ë¡ëœ ID: ê³ ì • ì„ê³„ì¹˜ ì‚¬ìš©
       - ê·¸ ì™¸: muÃ—1.1, 1.2, 1.3 ìë™ ì„ê³„ì¹˜ ì‚¬ìš©
    3) ë§ˆì§€ë§‰ ê°’ì˜ ë ˆë²¨ íŒë‹¨
       - level âˆˆ {normal, yellow, orange, red}
       - íŒ€ì¥ë‹˜ ìš”ì²­: ì£¼ì˜ ì´ìƒ(yellow, orange, red)ì€ ëª¨ë‘ 'Y', ê·¸ ì™¸ëŠ” 'G'
    ë°˜í™˜:
      flag: 'G' ë˜ëŠ” 'Y'
      info: ê°ì¢… ì°¸ê³  ì •ë³´(dict)
    """
    if len(s) < MIN_PERIODS:
        # ë°ì´í„°ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ë³´ìˆ˜ì ìœ¼ë¡œ G ì²˜ë¦¬
        return "G", {"reason": "too_short", "len": len(s)}

    roll_mean = s.rolling(window=window, min_periods=MIN_PERIODS).mean()
    roll_std = s.rolling(window=window, min_periods=MIN_PERIODS).std(ddof=0)

    last_val = s.iloc[-1]
    mu = roll_mean.iloc[-1]
    sd = roll_std.iloc[-1]

    if pd.isna(mu) or pd.isna(sd) or sd == 0:
        return "G", {
            "reason": "nan_or_zero_std",
            "mu": mu,
            "sd": sd,
            "last": last_val,
        }

    # z-scoreëŠ” ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ê³„ì‚°
    z = (last_val - mu) / sd

    # 1) ì„ê³„ì¹˜ ê²°ì • (ê³ ì •/ìë™)
    ind_id = (indicator_id or "").upper()
    cfg = CUSTOM_THRESHOLDS.get(ind_id)

    if cfg is not None:
        # âœ… ê³ ì • ì„ê³„ì¹˜ ì‚¬ìš©
        direction = cfg.get("direction", "up")
        thr_yellow = cfg["yellow"]
        thr_orange = cfg["orange"]
        thr_red = cfg["red"]
        reason = "custom_threshold"
    else:
        # âœ… ìë™ ì„ê³„ì¹˜ (í‰ê·  Ã— ë¹„ìœ¨)
        direction = "up"  # ê¸°ë³¸: ê°’ì´ ì˜¬ë¼ê°ˆìˆ˜ë¡ ìœ„í—˜í•œ ì§€í‘œë¡œ ê°€ì •
        thr_yellow = mu * 1.1
        thr_orange = mu * 1.2
        thr_red = mu * 1.3
        reason = "auto_threshold"

    # 2) ë ˆë²¨ íŒì •
    if direction == "up":
        if last_val >= thr_red:
            level = "red"
        elif last_val >= thr_orange:
            level = "orange"
        elif last_val >= thr_yellow:
            level = "yellow"
        else:
            level = "normal"
    else:  # direction == "down": ê°’ì´ ì‘ì•„ì§ˆìˆ˜ë¡ ìœ„í—˜
        if last_val <= thr_red:
            level = "red"
        elif last_val <= thr_orange:
            level = "orange"
        elif last_val <= thr_yellow:
            level = "yellow"
        else:
            level = "normal"

    # 3) íŒ€ì¥ë‹˜ ìš”ì²­: "ì£¼ì˜ ì´ìƒì€ Y" â†’ normalë§Œ G, ë‚˜ë¨¸ì§€ëŠ” ì „ë¶€ Y
    if level == "normal":
        flag = "G"
    else:
        flag = "Y"

    info = {
        "last": last_val,
        "mu": mu,
        "sd": sd,
        "zscore": z,
        "thr_yellow": thr_yellow,
        "thr_orange": thr_orange,
        "thr_red": thr_red,
        "level": level,
        "reason": reason,
    }

    # ì˜ˆì „ êµ¬ì¡° í˜¸í™˜ìš© í•„ë“œ(upper/lower)ëŠ” Noneìœ¼ë¡œ ë‘ 
    info["upper"] = None
    info["lower"] = None

    return flag, info


# ---------------- ì—‘ì…€ ê¸°ë¡ ----------------
def write_results_to_excel(
    path: str,
    flag: str,
    mu: Optional[float],
    sd: Optional[float],
    sheet_name=0,
):
    """
    ì—‘ì…€ íŒŒì¼ì˜ C1/D1/E1 ì—…ë°ì´íŠ¸:
      - C1: G ë˜ëŠ” Y (ì£¼ì˜ ì´ìƒì€ ëª¨ë‘ Y)
      - D1: 60ì¼ ë¡¤ë§ í‰ê· (mu)
      - E1: 60ì¼ ë¡¤ë§ í‘œì¤€í¸ì°¨(sd = Ïƒ)
    """
    from openpyxl import load_workbook

    wb = load_workbook(path)
    ws = wb[wb.sheetnames[sheet_name]] if isinstance(sheet_name, int) else wb[sheet_name]

    ws["C1"] = flag

    def _num_or_none(x):
        try:
            if x is None:
                return None
            if isinstance(x, (int, float)) and not (
                isinstance(x, float) and np.isnan(x)
            ):
                return float(x)
            return None
        except Exception:
            return None

    ws["D1"] = _num_or_none(mu)  # 60ì¼ í‰ê· 
    ws["E1"] = _num_or_none(sd)  # 60ì¼ í‘œì¤€í¸ì°¨(Ïƒ)

    wb.save(path)


# ---------------- ë©”ì¸ ë“œë¼ì´ë²„ ----------------
def main():
    results = []
    for i in FILE_RANGE:
        fname = f"{FILE_PREFIX}{i:03d}.xlsx"
        fpath = os.path.join(BASE_DIR, fname)
        if not os.path.exists(fpath):
            results.append((fname, "missing", None))
            print(f"[SKIP] {fname} (file not found)")
            continue

        try:
            s, _df = _load_timeseries(fpath, sheet_name=SHEET_NAME)

            # âœ… [ë³€ê²½] ì§€í‘œ IDë¥¼ ë„˜ê²¨ì„œ ì„ê³„ì¹˜ ê¸°ë°˜ í”Œë˜ê·¸ ê³„ì‚°
            indicator_id = f"{FILE_PREFIX}{i:03d}"
            flag, info = compute_threshold_flag(
                s, indicator_id=indicator_id, window=WINDOW, k=K
            )

            mu = info.get("mu") if isinstance(info, dict) else None
            sd = info.get("sd") if isinstance(info, dict) else None
            write_results_to_excel(fpath, flag, mu, sd, sheet_name=SHEET_NAME)

            ztxt = (
                f"{info.get('zscore'):.2f}"
                if isinstance(info, dict)
                and "zscore" in info
                and pd.notna(info.get("zscore"))
                else "n/a"
            )
            mutxt = (
                f"{mu:.6g}"
                if isinstance(mu, (int, float)) and not pd.isna(mu)
                else "n/a"
            )
            sdtxt = (
                f"{sd:.6g}"
                if isinstance(sd, (int, float)) and not pd.isna(sd)
                else "n/a"
            )

            level = info.get("level") if isinstance(info, dict) else "n/a"

            print(
                f"[OK] {fname} -> C1='{flag}', level={level}, "
                f"D1(mu)={mutxt}, E1(sd)={sdtxt} (z={ztxt})"
            )
            results.append((fname, flag, info))
        except Exception as e:
            err_msg = str(e)
            print(f"[ERR] {fname}: {err_msg}")
            # ê°„ë‹¨ í”„ë¡œë¸Œ: ì»¬ëŸ¼/íƒ€ì… íŒíŠ¸ ì¶œë ¥ (í—¤ë”/í˜•ì‹ ì¶”ì )
            try:
                df_probe = detect_header_and_read(fpath, sheet_name=SHEET_NAME)
                print(f"  -> columns: {list(df_probe.columns)}")
                sample_info = []
                for c in df_probe.columns[:10]:
                    sample_info.append(f"{c}:{str(df_probe[c].dtype)}")
                print("  -> dtypes(head):", ", ".join(sample_info))
            except Exception as e2:
                print(f"  -> probe failed: {e2}")
            results.append((fname, "error", err_msg))

    # ìš”ì•½ CSV ì €ì¥
    summary_path = os.path.join(
        BASE_DIR,
        f"vol_band_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
    )
    rows = []
    for fname, flag, info in results:
        row = {"file": fname, "flag": flag}
        if isinstance(info, dict):
            row.update(
                {
                    "last": info.get("last"),
                    "mu": info.get("mu"),
                    "sd": info.get("sd"),
                    "upper": info.get("upper"),
                    "lower": info.get("lower"),
                    "zscore": info.get("zscore"),
                    "thr_yellow": info.get("thr_yellow"),
                    "thr_orange": info.get("thr_orange"),
                    "thr_red": info.get("thr_red"),
                    "level": info.get("level"),
                    "reason": info.get("reason"),
                }
            )
        elif isinstance(info, str):
            row["error"] = info
        rows.append(row)
    pd.DataFrame(rows).to_csv(summary_path, index=False, encoding="utf-8-sig")
    print(f"\nìš”ì•½ ì €ì¥: {summary_path}")


if __name__ == "__main__":
    main()


# -*- coding: utf-8 -*-
r"""
IND001~IND151 ì—‘ì…€ë§Œ ì‚¬ìš©í•˜ì—¬ íŒŒìƒì§€í‘œë¥¼ ê³„ì‚°í•˜ê³ 
id, data(=JSON ë¬¸ìì—´) 2-ì»¬ëŸ¼ í˜•ì‹ìœ¼ë¡œ í•˜ë‚˜ì˜ ì—‘ì…€ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.

ì €ì¥ ìœ„ì¹˜:
  C:\Users\amongpapa\chartup\go_scen\data\market_data\market_data.xlsx

ì„¤ì¹˜:
  pip install pandas numpy openpyxl
"""

import os
import re
import json
import math
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Optional, Tuple, Dict

# ================= íŒ€ì¥ë‹˜ í™˜ê²½ ì„¤ì • =================
BASE_DIR   = r"C:\Users\amongpapa\chartup\go_scen\data\set"                  # IND ì—‘ì…€ í´ë”
OUT_PATH   = r"C:\Users\amongpapa\chartup\go_scen\data\market_data\market_data.xlsx"
FILE_PREFIX = "IND"
FILE_RANGE  = range(1, 152)                                                  # 001~151
SHEET_NAME  = 0

WINDOWS_RET = [20, 60, 120]
ANNUAL_DAYS = 252
VOL_QUANTILES = (0.33, 0.66)

# ìƒê´€/ë² íƒ€ìš© ë²¤ì¹˜ë§ˆí¬ ë§¤í•‘(í•„ìš” ì‹œ ì±„ìš°ì„¸ìš”: ì˜ˆ 'KOSPI': 'IND001')
BENCHMARK_MAP: Dict[str, str] = {}

DATE_COL_HINTS  = ["date", "ë‚ ì§œ", "ì¼ì", "time", "ì¼ì‹œ"]
VALUE_COL_HINTS = ["close", "price", "value", "index", "ì§€ìˆ˜", "ì¢…ê°€", "ê°€ê²©", "ê°’", "ìˆ˜ì¹˜"]

MIN_PERIODS = 30  # ì§§ì€ ë°ì´í„° ë³´í˜¸
# ====================================================


# ---------------- ìœ í‹¸: ì—‘ì…€ í—¤ë” ìë™ ê°ì§€ ----------------
def detect_header_and_read(path, sheet_name=0, max_scan=10) -> pd.DataFrame:
    """
    í—¤ë”/ë°ì´í„° ì‹œì‘í–‰ ë¶ˆëª…í™• ì—‘ì…€ì„ ì•ˆì „í•˜ê²Œ ì½ê¸°:
      1) ì¼ë°˜ ë¡œë“œ í›„ í’ˆì§ˆ ì ê²€, í†µê³¼ì‹œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
      2) ìœ„ì—ì„œë¶€í„° max_scanì¤„ì„ í—¤ë” í›„ë³´ë¡œ ìŠ¤ì½”ì–´ë§í•´ ìµœì  í—¤ë” ì±„íƒ
    """
    try:
        df0 = pd.read_excel(path, sheet_name=sheet_name, engine="openpyxl", header=0)
    except Exception:
        df0 = pd.read_excel(path, sheet_name=sheet_name, engine="openpyxl", header=None)

    if df0.shape[0] == 0:
        df0 = pd.read_excel(path, sheet_name=sheet_name, engine="openpyxl", header=None)

    if _likely_good(df0):
        return _clean_columns(df0)

    raw = pd.read_excel(path, sheet_name=sheet_name, engine="openpyxl", header=None)
    nrows = min(max_scan, len(raw))
    best_score, best_row = -1, 0
    for hdr in range(nrows):
        tmp = raw.copy()
        tmp.columns = tmp.iloc[hdr].astype(str).str.strip()
        tmp = tmp.iloc[hdr + 1 :].reset_index(drop=True)
        score = _numeric_score(tmp, lookahead=5)
        if score > best_score:
            best_score, best_row = score, hdr

    raw.columns = raw.iloc[best_row].astype(str).str.strip()
    df = raw.iloc[best_row + 1 :].reset_index(drop=True)
    return _clean_columns(df)

def _likely_good(df: pd.DataFrame) -> bool:
    return _numeric_score(df, lookahead=5) >= 2

def _numeric_score(df: pd.DataFrame, lookahead: int = 5) -> int:
    head = df.head(lookahead)
    cnt = 0
    for c in head.columns:
        s = head[c]
        s_num = _as_numeric_series(s, strict=False)
        if s_num.notna().mean() >= 0.5:
            cnt += 1
    return cnt

def _clean_columns(df: pd.DataFrame) -> pd.DataFrame:
    df.columns = [re.sub(r"\s+", " ", str(c)).strip() for c in df.columns]
    return df


# ---------------- ìˆ«ì ë¬¸ìì—´ â†’ float ì‹œë¦¬ì¦ˆ(ê°•ë ¥ ë°©ì–´) ----------------
def _as_numeric_series(s: pd.Series, strict: bool = True) -> pd.Series:
    """
    ë¬¸ì ì„ì¸ ìˆ«ìë¥¼ floatë¡œ ì •ì œ(%, ê´„í˜¸ìŒìˆ˜, ì½¤ë§ˆ ë“±)í•˜ì—¬ í•­ìƒ Series ë°˜í™˜.
    strict=False ì´ë©´ ì˜ˆì™¸ëŠ” ì¡°ìš©íˆ NaNìœ¼ë¡œ ë³´ëƒ„.
    """
    try:
        # ì´ë¯¸ ìˆ«ìí˜•ì´ë©´ ì¡°ìš©íˆ ë³€í™˜
        if pd.api.types.is_numeric_dtype(s):
            return pd.to_numeric(s, errors="coerce")

        # ì–´ë–¤ íƒ€ì…ì´ë“  ë¬¸ìì—´ dtypeìœ¼ë¡œ ê°•ì œ(Series.str ì‚¬ìš© ë³´ì¥)
        ss = s.astype("string")  # pandas StringDtype
        ss = ss.str.strip()

        # ë¹ˆê°’/ëŒ€ì‹œë¥˜ â†’ NaN
        ss = ss.replace({"": pd.NA, "-": pd.NA, "â€”": pd.NA, "_": pd.NA, "nan": pd.NA, "NaN": pd.NA, "None": pd.NA})

        # ê´„í˜¸ ìŒìˆ˜
        neg_mask = ss.str.match(r"^\(.*\)$", na=False)
        ss2 = ss.str.replace(r"^\((.*)\)$", r"\1", regex=True)

        # ì½¤ë§ˆ ì œê±°, % ì œê±°
        ss2 = ss2.str.replace(",", "", regex=False)
        pct_mask = ss2.str.endswith("%", na=False)
        ss2 = ss2.str.replace("%", "", regex=False)

        num = pd.to_numeric(ss2, errors="coerce")

        # ê´„í˜¸ ìŒìˆ˜ ì ìš©
        if neg_mask.any():
            idx = neg_mask & num.notna()
            num.loc[idx] = -num.loc[idx].abs()

        # í¼ì„¼íŠ¸ â†’ /100
        if pct_mask.any():
            idx = pct_mask & num.notna()
            num.loc[idx] = num.loc[idx] / 100.0

        # í•­ìƒ Series
        if not isinstance(num, pd.Series):
            num = pd.Series(num, index=s.index, dtype="float64")
        return num.astype(float)

    except Exception:
        if strict:
            raise
        # ë¬¸ì œ ìˆìœ¼ë©´ ì „ë¶€ NaNìœ¼ë¡œ ë¦¬í„´(ìŠ¤ì½”ì–´ë§ìš©)
        return pd.to_numeric(pd.Series([pd.NA]*len(s), index=s.index), errors="coerce")


# ---------------- ë‚ ì§œ/ê°’ ì»¬ëŸ¼ ìë™ ì„ íƒ ----------------
def _pick_date_col(df: pd.DataFrame) -> Optional[str]:
    for hint in DATE_COL_HINTS:
        for c in df.columns:
            if hint.lower() in str(c).lower():
                return c
    dt_cols = [c for c in df.columns if pd.api.types.is_datetime64_any_dtype(df[c])]
    if dt_cols:
        return dt_cols[0]
    best_col, best_ratio = None, 0.0
    for c in df.columns:
        try:
            converted = pd.to_datetime(df[c], errors="coerce", infer_datetime_format=True)
            ratio = converted.notna().mean()
            if ratio > best_ratio and ratio >= 0.7:
                best_col, best_ratio = c, ratio
        except Exception:
            pass
    return best_col

def _pick_value_col(df: pd.DataFrame, exclude_cols: list) -> Optional[str]:
    # íŒíŠ¸ ìš°ì„ 
    for hint in VALUE_COL_HINTS:
        for c in df.columns:
            if c in exclude_cols: 
                continue
            if hint.lower() in str(c).lower():
                return c

    # ìˆ«ìí˜• í›„ë³´ ìŠ¤ì½”ì–´ë§
    candidates = []
    for c in df.columns:
        if c in exclude_cols:
            continue
        try:
            series = _as_numeric_series(df[c], strict=False)
            na_ratio = pd.isna(series).mean()
            var = np.nanvar(series.astype(float))
            if (~pd.isna(series)).sum() >= max(MIN_PERIODS, 10) and var > 0:
                candidates.append((c, na_ratio, var))
        except Exception:
            continue

    if not candidates:
        return None

    candidates.sort(key=lambda x: (x[1], -x[2]))  # ê²°ì¸¡â†“, ë¶„ì‚°â†‘
    return candidates[0][0]


# ---------------- ì‹œê³„ì—´ ë¡œë”© ----------------
def load_series_from_excel(path: str, sheet_name=0) -> pd.Series:
    """ì—‘ì…€ì—ì„œ (ë‚ ì§œ, ê°’) ì‹œê³„ì—´ Series ë°˜í™˜ (DatetimeIndex ì •ë ¬)"""
    df = detect_header_and_read(path, sheet_name=sheet_name)

    date_col = _pick_date_col(df)
    value_col = _pick_value_col(df, exclude_cols=[date_col] if date_col else [])
    if value_col is None:
        raise ValueError("ìˆ«ìí˜• ì‹œê³„ì—´ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    if date_col is not None:
        dt = pd.to_datetime(df[date_col], errors="coerce", infer_datetime_format=True)
        df = df.loc[dt.notna()].copy()
        df.index = pd.to_datetime(df[date_col], errors="coerce")
        df = df.sort_index()
    else:
        df = df.reset_index(drop=True)

    s = _as_numeric_series(df[value_col]).astype(float).dropna()

    # ê°™ì€ ë‚ ì§œ ì¤‘ë³µ â†’ í‰ê· 
    if isinstance(df.index, pd.DatetimeIndex) and s.index.has_duplicates:
        s = s.groupby(level=0).mean()

    s = s.sort_index()
    if len(s) < MIN_PERIODS:
        raise ValueError(f"ë°ì´í„° ê¸¸ì´ ë¶€ì¡±(len={len(s)})")

    return s


# ---------------- ë³´ì¡° ê³„ì‚° í•¨ìˆ˜ ----------------
def pct_return(s: pd.Series, days: int) -> Optional[float]:
    if len(s) <= days:
        return None
    return float(s.iloc[-1] / s.iloc[-days-1] - 1.0)

def ann_return_from_period(r: Optional[float], days: int) -> Optional[float]:
    if r is None:
        return None
    try:
        return float((1.0 + r) ** (ANNUAL_DAYS / days) - 1.0)
    except Exception:
        return None

def rolling_vol_annualized(s: pd.Series, days: int) -> Optional[float]:
    if len(s) < max(days+1, MIN_PERIODS):
        return None
    rets = s.pct_change()
    vol = rets.rolling(window=days, min_periods=int(days*0.6)).std(ddof=0)
    last_vol = vol.iloc[-1]
    if pd.isna(last_vol):
        return None
    return float(last_vol * np.sqrt(ANNUAL_DAYS))

def drawdown_stats(s: pd.Series, lookback_days: int) -> Tuple[Optional[float], Optional[float]]:
    if len(s) < max(lookback_days, MIN_PERIODS):
        return (None, None)
    sub = s.iloc[-lookback_days:]
    peak = sub.cummax()
    dd = sub / peak - 1.0
    mdd = dd.min()
    curr_dd = dd.iloc[-1]
    return (float(mdd), float(curr_dd))

def sma(s: pd.Series, days: int) -> Optional[float]:
    if len(s) < days:
        return None
    return float(s.rolling(days).mean().iloc[-1])

def ema(s: pd.Series, days: int) -> Optional[float]:
    if len(s) < days:
        return None
    return float(s.ewm(span=days, adjust=False).mean().iloc[-1])

def gap_pct(price: float, ref: Optional[float]) -> Optional[float]:
    if ref is None or ref == 0 or price is None:
        return None
    return float(price / ref - 1.0)

def golden_dead_cross_state(s: pd.Series, short=20, long=60) -> Optional[str]:
    if len(s) < long + 1:
        return None
    sma_s = s.rolling(short).mean()
    sma_l = s.rolling(long).mean()
    prev = np.sign(sma_s.iloc[-2] - sma_l.iloc[-2])
    curr = np.sign(sma_s.iloc[-1] - sma_l.iloc[-1])
    if prev <= 0 and curr > 0:
        return "golden"
    elif prev >= 0 and curr < 0:
        return "dead"
    else:
        return "none"

def slope_tstat_lastN(s: pd.Series, N: int = 60) -> Tuple[Optional[float], Optional[float]]:
    if len(s) < N:
        return (None, None)
    y = s.iloc[-N:].values.astype(float)
    x = np.arange(N).astype(float)
    x_mean = x.mean()
    y_mean = y.mean()
    cov_xy = np.sum((x - x_mean) * (y - y_mean))
    var_x = np.sum((x - x_mean) ** 2)
    if var_x == 0:
        return (None, None)
    beta1 = cov_xy / var_x
    y_hat = (beta1 * (x - x_mean)) + y_mean
    resid = y - y_hat
    s2 = np.sum(resid**2) / (N - 2)
    se_beta1 = math.sqrt(s2 / var_x)
    tstat = beta1 / se_beta1 if se_beta1 != 0 else None
    return (float(beta1), float(tstat) if tstat is not None else None)

def pos_in_52w(s: pd.Series) -> Optional[float]:
    if len(s) < 20:
        return None
    look = s.iloc[-min(len(s), 252):]
    hi, lo = float(look.max()), float(look.min())
    last = float(look.iloc[-1])
    if hi == lo:
        return None
    return float((last - lo) / (hi - lo))

def vol_regime_label(s: pd.Series, days: int = 60) -> Optional[str]:
    if len(s) < max(days+20, MIN_PERIODS+20):
        return None
    rets = s.pct_change()
    vol = rets.rolling(window=days, min_periods=int(days*0.6)).std(ddof=0) * np.sqrt(ANNUAL_DAYS)
    curr = vol.iloc[-1]
    hist = vol.dropna().values
    if np.isnan(curr) or len(hist) < 30:
        return None
    q1, q2 = np.quantile(hist, VOL_QUANTILES)
    if curr < q1:
        return "low"
    elif curr < q2:
        return "mid"
    else:
        return "high"

def trend_label_from_slope(s: pd.Series, N: int = 60) -> Optional[str]:
    slope, t = slope_tstat_lastN(s, N)
    if slope is None or t is None:
        return None
    if t > 2:
        return "up"
    elif t < -2:
        return "down"
    else:
        return "flat"

def compute_beta(x: pd.Series, mkt: pd.Series, days: int = 60) -> Optional[float]:
    if len(x) < days+1 or len(mkt) < days+1:
        return None
    rx = x.pct_change().iloc[-days:]
    rm = mkt.pct_change().iloc[-days:]
    df = pd.concat([rx, rm], axis=1).dropna()
    if len(df) < int(days*0.6):
        return None
    cov = np.cov(df.iloc[:,0], df.iloc[:,1])[0,1]
    var = np.var(df.iloc[:,1])
    if var == 0:
        return None
    return float(cov / var)

def corr_rolling(x: pd.Series, y: pd.Series, days: int = 60) -> Optional[float]:
    if len(x) < days+1 or len(y) < days+1:
        return None
    rx = x.pct_change()
    ry = y.pct_change()
    corr = rx.rolling(days).corr(ry)
    val = corr.iloc[-1]
    return float(val) if pd.notna(val) else None


# ---------------- ë©”ì¸ íŒŒì´í”„ë¼ì¸ ----------------
def main():
    series_map: Dict[str, pd.Series] = {}
    for i in FILE_RANGE:
        fid = f"{FILE_PREFIX}{i:03d}"
        fpath = os.path.join(BASE_DIR, f"{fid}.xlsx")
        if not os.path.exists(fpath):
            print(f"[SKIP] {fid}: file not found")
            continue
        try:
            s = load_series_from_excel(fpath, sheet_name=SHEET_NAME)
            series_map[fid] = s
        except Exception as e:
            # í•µì‹¬: ë¬¸ìì—´ ì²˜ë¦¬/ì»¬ëŸ¼ë§¤í•‘ ì‹¤íŒ¨ ì‹œì—ë„ ì›ì¸ ì¶œë ¥í•˜ê³  ê³„ì†
            print(f"[SKIP] {fid}: {e}")

    if not series_map:
        raise RuntimeError("ìœ íš¨í•œ IND ì‹œê³„ì—´ì„ í•˜ë‚˜ë„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # ë²¤ì¹˜ë§ˆí¬ í•´ì„(ì˜µì…˜)
    bench_series: Dict[str, pd.Series] = {}
    for name, ind in BENCHMARK_MAP.items():
        if ind in series_map:
            bench_series[name] = series_map[ind]

    rows = []
    now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    for fid, s in series_map.items():
        last = float(s.iloc[-1])
        asof = str(s.index[-1]) if isinstance(s.index, pd.DatetimeIndex) else now_str

        # ë³€ë™ì„±/ìˆ˜ìµë¥ 
        rets = {}; rets_ann = {}; vols = {}
        for d in WINDOWS_RET:
            r = pct_return(s, d)
            rets[f"{d}d"] = r
            rets_ann[f"{d}d_ann"] = ann_return_from_period(r, d)
            vols[f"{d}d_vol_ann"] = rolling_vol_annualized(s, d)

        # ì„±ê³¼(YTD/1M/3M/6M/1Y)
        period_map = {"1m":21, "3m":63, "6m":126, "1y":252}
        perf = {k: pct_return(s, d) for k, d in period_map.items()}
        if isinstance(s.index, pd.DatetimeIndex):
            year = s.index[-1].year
            ytd_slice = s[s.index.year == year]
            ytd_first = ytd_slice.iloc[0] if len(ytd_slice) > 0 else None
            perf["ytd"] = float(last / ytd_first - 1.0) if ytd_first and ytd_first != 0 else None
        else:
            perf["ytd"] = None

        # ë‚™í­
        mdd6, dd6   = drawdown_stats(s, 126)
        mdd1y, dd1y = drawdown_stats(s, 252)

        # ì¶”ì„¸/ê´´ë¦¬
        sma20, sma60, sma120 = sma(s,20), sma(s,60), sma(s,120)
        ema20, ema60, ema120 = ema(s,20), ema(s,60), ema(s,120)
        gaps = {
            "gap_to_sma20":  gap_pct(last, sma20),
            "gap_to_sma60":  gap_pct(last, sma60),
            "gap_to_sma120": gap_pct(last, sma120),
            "gap_to_ema20":  gap_pct(last, ema20),
            "gap_to_ema60":  gap_pct(last, ema60),
            "gap_to_ema120": gap_pct(last, ema120),
        }
        cross_20_60  = golden_dead_cross_state(s, 20, 60)
        cross_60_120 = golden_dead_cross_state(s, 60, 120)

        # ê¸°ìš¸ê¸°/t
        slope60, t60 = slope_tstat_lastN(s, 60)

        # 52ì£¼ ìœ„ì¹˜
        pos52w = pos_in_52w(s)
        pos52w_pct = float(pos52w*100.0) if pos52w is not None else None

        # ë ˆì§
        vol_reg   = vol_regime_label(s, 60)
        trend_reg = trend_label_from_slope(s, 60)
        regime    = f"{trend_reg}-{vol_reg}" if trend_reg and vol_reg else None

        # ìƒê´€/ë² íƒ€(ì˜µì…˜)
        corr_res = {}
        beta_res = {}
        for bname, bs in bench_series.items():
            corr_res[bname] = corr_rolling(s, bs, 60)
        mkt_key = next((k for k in ["KOSPI", "S&P500", "KOSDAQ"] if k in bench_series), None)
        if mkt_key:
            beta_res[mkt_key] = compute_beta(s, bench_series[mkt_key], 60)

        # ìš”ì•½ ìµœê·¼ê°’
        latest = {"last": last, "asof": asof, "ytd": perf.get("ytd"),
                  "1m": perf.get("1m"), "3m": perf.get("3m"),
                  "6m": perf.get("6m"), "1y": perf.get("1y"), "note": None}

        # === id, data(JSON) ì €ì¥ ===
        def add_row(suffix: str, obj):
            rows.append({"id": f"{fid}:{suffix}", "data": json.dumps(obj, ensure_ascii=False)})

        add_row("returns", {**rets, **rets_ann})
        add_row("vol", vols)
        add_row("mdd", {"mdd_6m": mdd6, "dd_6m": dd6, "mdd_1y": mdd1y, "dd_1y": dd1y})
        add_row("trend",
                {"sma20":sma20,"sma60":sma60,"sma120":sma120,
                 "ema20":ema20,"ema60":ema60,"ema120":ema120,
                 **gaps,
                 "cross_20_60": cross_20_60, "cross_60_120": cross_60_120})
        add_row("slope60", {"slope": slope60, "tstat": t60})
        add_row("pos_52w", {"pos_0to1": pos52w, "pos_pct": pos52w_pct})
        add_row("regime", {"trend": trend_reg, "vol": vol_reg, "regime": regime})
        if corr_res:
            add_row("corr60", corr_res)
        if beta_res:
            add_row("beta60", beta_res)
        add_row("latest", latest)

        summary = {
            "latest": latest,
            "returns": {**rets, **rets_ann},
            "vol": vols,
            "mdd": {"mdd_6m": mdd6, "dd_6m": dd6, "mdd_1y": mdd1y, "dd_1y": dd1y},
            "trend": {"gaps": gaps, "cross_20_60": cross_20_60, "cross_60_120": cross_60_120},
            "slope60": {"slope": slope60, "tstat": t60},
            "pos_52w_pct": pos52w_pct,
            "regime": {"trend": trend_reg, "vol": vol_reg, "regime": regime},
            "corr60": corr_res if corr_res else None,
            "beta60": beta_res if beta_res else None
        }
        add_row("summary", summary)

    # ì €ì¥
    os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)
    df_out = pd.DataFrame(rows, columns=["id", "data"])
    df_out.to_excel(OUT_PATH, index=False, engine="openpyxl")
    print(f"[OK] ì €ì¥ ì™„ë£Œ: {OUT_PATH} (rows={len(df_out)})")


if __name__ == "__main__":
    main()


# -*- coding: utf-8 -*-
"""
ëª©ì :
- C:/Users/amongpapa/chartup/raw_data í´ë”ì˜ risk_thresholds_YYYYMMDD.xlsx íŒŒì¼ë“¤ ì¤‘,
  íŒŒì¼ëª… ë‚ ì§œ(YYYYMMDD)ê°€ 'ê°€ì¥ ìµœì‹ 'ì¸ íŒŒì¼ì„ ìë™ ì„ íƒ
- í•´ë‹¹ íŒŒì¼ì˜ alerts ì‹œíŠ¸ë¥¼ ì½ì–´ì„œ,
  ì—‘ì…€ 2í–‰ â†’ IND500, 3í–‰ â†’ IND501, ... ê·œì¹™ìœ¼ë¡œ ë§¤í•‘
- ê° í–‰ì— 'TRUE'ê°€ í•œ ì¹¸ì´ë¼ë„ ìˆìœ¼ë©´ C1='Y', ì•„ë‹ˆë©´ 'G'ë¥¼
  C:/Users/amongpapa/chartup/go_scen/data/set/IND###.xlsx ì— ê¸°ë¡
- ëŒ€ìƒ ë²”ìœ„: IND500 ~ IND700 (ì¡´ì¬í•˜ëŠ” íŒŒì¼ë§Œ ì—…ë°ì´íŠ¸)

ì„¤ì¹˜:
    pip install pandas openpyxl

íŒ€ì¥ë‹˜ í™˜ê²½ ìœ ì˜:
- ìœˆë„ìš° ê²½ë¡œëŠ” pathlib.Path(r"...") í˜•ì‹ ì‚¬ìš©
- ìŠ¬ë˜ì‹œ(/)ë¥¼ ì¨ë„ ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” Pathë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì•ˆì „í•©ë‹ˆë‹¤.
"""

from pathlib import Path  # âœ… ìœˆë„ìš°ì—ì„œë„ ì•ˆì „í•œ ê²½ë¡œ ì²˜ë¦¬
from datetime import datetime
import re
import pandas as pd
from openpyxl import load_workbook

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) ê²½ë¡œ/íŒŒë¼ë¯¸í„° ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# risk_thresholds ì—‘ì…€ë“¤ì´ ìˆëŠ” í´ë”
RISK_DIR = Path(r"C:\Users\amongpapa\chartup\raw_data")

# alerts ì‹œíŠ¸ëª…
ALERTS_SHEET = "alerts"

# IND ì—‘ì…€ë“¤ì´ ìˆëŠ” í´ë”(ì—…ë°ì´íŠ¸ ëŒ€ìƒ)
TARGET_DIR = Path(r"C:\Users\amongpapa\chartup\go_scen\data\set")

# ë§¤í•‘: ì—‘ì…€ '2í–‰' â†’ IND500 (ì¦‰, row 2 => 500, row 3 => 501, ...)
START_EXCEL_ROW = 2
START_IND = 500
MAX_IND = 700  # ì´ ê°’ê¹Œì§€ ì¡´ì¬í•˜ëŠ” íŒŒì¼ë§Œ ì—…ë°ì´íŠ¸

# ì—…ë°ì´íŠ¸í•  ì›Œí¬ì‹œíŠ¸: ì²« ì‹œíŠ¸ë¥¼ ì‚¬ìš©(ì •ìˆ˜ 0). íŠ¹ì • ì‹œíŠ¸ëª…ì´ë©´ ë¬¸ìì—´ë¡œ ì§€ì • ê°€ëŠ¥.
TARGET_SHEET = 0

# 'TRUE' íŒì • ì‹œ í¬í•¨í•  ê°’(ëŒ€ì†Œë¬¸ì, ê³µë°± ëŒ€ì‘)
TRUE_TOKENS = {"TRUE"}  # ë¬¸ìì—´ 'TRUE' ë˜ëŠ” ë¶ˆë¦¬ì–¸ Trueë¥¼ í—ˆìš©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


# ---------------- ìµœì‹  íŒŒì¼ ì„ íƒ: íŒŒì¼ëª… ë‚ ì§œ(YYYYMMDD) ê¸°ì¤€ ----------------
def pick_risk_file_by_name_date() -> Path:
    """
    risk_thresholds_YYYYMMDD.xlsx íŒŒì¼ë“¤ì—ì„œ 'íŒŒì¼ëª… ë‚ ì§œ'ê°€ ê°€ì¥ ìµœì‹ ì¸ íŒŒì¼ì„ ì„ íƒ.
    ì˜ˆ: risk_thresholds_20251006.xlsx ê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ì‚¬ìš©.
    """
    patt = re.compile(r"^risk_thresholds_(\d{8})\.xlsx$", re.IGNORECASE)
    best = None
    best_dt = None
    for p in RISK_DIR.glob("risk_thresholds_*.xlsx"):
        m = patt.match(p.name)
        if not m:
            continue
        try:
            dt = datetime.strptime(m.group(1), "%Y%m%d")
        except ValueError:
            continue
        if (best_dt is None) or (dt > best_dt):
            best_dt = dt
            best = p
    if best is None:
        raise FileNotFoundError(f"í´ë”ì— risk_thresholds_YYYYMMDD.xlsx íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {RISK_DIR}")
    return best


# ---------------- ì—‘ì…€ í–‰ â†’ IND ë²ˆí˜¸ ë§¤í•‘ ----------------
def excel_row_to_ind(excel_row: int) -> int:
    """
    ì—‘ì…€ ì‹¤ì œ í–‰ ë²ˆí˜¸(1-based) â†’ IND ë²ˆí˜¸ ë§¤í•‘
    ì˜ˆ) 2í–‰ â†’ 500, 3í–‰ â†’ 501 ...
    """
    return START_IND + (excel_row - START_EXCEL_ROW)


# ---------------- í•œ í–‰ì— TRUE ì¡´ì¬ ì—¬ë¶€ ê²€ì‚¬ ----------------
def row_has_true(row_values) -> bool:
    """
    í•œ í–‰ì— TRUEê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ True ë°˜í™˜.
    - ë¶ˆë¦¬ì–¸ True
    - ë¬¸ìì—´ 'TRUE' (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ, ì•ë’¤ ê³µë°± ë¬´ì‹œ)
    """
    for v in row_values:
        # ë¶ˆë¦¬ì–¸ True
        if v is True:
            return True
        # ë¬¸ìì—´ 'TRUE'
        if isinstance(v, str) and v.strip().upper() in TRUE_TOKENS:
            return True
    return False


# ---------------- IND ì—‘ì…€ì˜ C1 ì—…ë°ì´íŠ¸ ----------------
def update_c1_flag(ind_num: int, flag: str) -> bool:
    """
    IND íŒŒì¼ì˜ C1ì„ flag('Y'/'G')ë¡œ ì—…ë°ì´íŠ¸.
    - íŒŒì¼ì´ ì—†ìœ¼ë©´ False ë°˜í™˜(ìŠ¤í‚µ)
    - ì„±ê³µí•˜ë©´ True
    """
    fpath = TARGET_DIR / f"IND{ind_num:03d}.xlsx"
    if not fpath.exists():
        print(f"  â­ï¸ ìŠ¤í‚µ (íŒŒì¼ì—†ìŒ): {fpath.name}")
        return False

    wb = load_workbook(fpath)
    ws = wb.worksheets[TARGET_SHEET] if isinstance(TARGET_SHEET, int) else wb[TARGET_SHEET]
    ws["C1"] = flag  # C1ë§Œ ê¸°ë¡ (D1/E1ì€ ë³€ê²½í•˜ì§€ ì•ŠìŒ)
    wb.save(fpath)
    print(f"  âœ… C1='{flag}' ê¸°ë¡: {fpath.name}")
    return True


# ---------------- ë©”ì¸ ë£¨í‹´ ----------------
def main():
    # 1) ìµœì‹  risk_thresholds íŒŒì¼ ì„ íƒ(íŒŒì¼ëª… ë‚ ì§œ ê¸°ì¤€)
    risk_file = pick_risk_file_by_name_date()
    print(f"â–¶ ê¸°ì¤€ íŒŒì¼(íŒŒì¼ëª… ë‚ ì§œ ìµœì‹ ): {risk_file.name}")

    # 2) alerts ì‹œíŠ¸ë¥¼ í—¤ë” ì—†ì´(raw) ë¡œë“œ â†’ ì—‘ì…€ ì‹¤ì œ í–‰ ë²ˆí˜¸ì™€ 1:1 ë§¤í•‘ ê°€ëŠ¥
    df = pd.read_excel(risk_file, sheet_name=ALERTS_SHEET, header=None, engine="openpyxl")
    nrows = df.shape[0]

    total_updated = 0
    total_skipped = 0

    # 3) ì—‘ì…€ '2í–‰'ë¶€í„° ëê¹Œì§€ ìˆœíšŒí•˜ë©° IND ë²ˆí˜¸ë¡œ ë§¤í•‘
    #    ë‹¨, IND ë²ˆí˜¸ê°€ MAX_INDë¥¼ ë„˜ìœ¼ë©´ ì¤‘ë‹¨
    for excel_row in range(START_EXCEL_ROW, nrows + 1):
        ind_num = excel_row_to_ind(excel_row)

        if ind_num < START_IND:
            continue
        if ind_num > MAX_IND:
            break  # ë²”ìœ„ë¥¼ ë„˜ìœ¼ë©´ ì¢…ë£Œ

        row_vals = df.iloc[excel_row - 1].tolist()  # pandasëŠ” 0-based â†’ excel_row-1
        has_true = row_has_true(row_vals)
        flag = "Y" if has_true else "G"

        print(f"[í–‰ {excel_row} â†’ IND{ind_num:03d}] TRUEì¡´ì¬={has_true} â†’ C1='{flag}'")
        ok = update_c1_flag(ind_num, flag)
        if ok:
            total_updated += 1
        else:
            total_skipped += 1

    print("\nâ”€â”€â”€â”€â”€â”€â”€â”€ ìš”ì•½ â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"ì—…ë°ì´íŠ¸ ì„±ê³µ: {total_updated}ê°œ")
    print(f"ìŠ¤í‚µ(íŒŒì¼ì—†ìŒ ë“±): {total_skipped}ê°œ")
    print("ì™„ë£Œ âœ…")


if __name__ == "__main__":
    main()


import pandas as pd
from pathlib import Path

# ğŸ“‚ íŒŒì¼ ê²½ë¡œ
market_data_path = Path(r"C:\Users\amongpapa\chartup\go_scen\data\market_data\market_data.xlsx")
indicator_path   = Path(r"C:\Users\amongpapa\lm\agent\go_scen\data\indicator.xlsx")

# 1ï¸âƒ£ ì—‘ì…€ ë¶ˆëŸ¬ì˜¤ê¸°
market_df = pd.read_excel(market_data_path)
indicator_df = pd.read_excel(indicator_path)

# 2ï¸âƒ£ id ì¹¼ëŸ¼ì—ì„œ "IND001:returns" â†’ "IND001" ë¶€ë¶„ ì¶”ì¶œ
market_df["Indicator_ID"] = market_df["id"].str.split(":").str[0]

# 3ï¸âƒ£ indicator.xlsx ë§¤í•‘
merged_df = market_df.merge(
    indicator_df[["Indicator_ID", "Indicator_Name", "Bloomberg_Ticker"]],
    on="Indicator_ID",
    how="left"
)

# 4ï¸âƒ£ C, Dì—´ ìœ„ì¹˜ì— Indicator_Name, Bloomberg_Ticker ì‚½ì…
cols = list(market_df.columns)
insert_position = 2  # Cì—´ ìœ„ì¹˜
for new_col in ["Indicator_Name", "Bloomberg_Ticker"]:
    cols.insert(insert_position, new_col)
    insert_position += 1

final_df = merged_df[cols]

# 5ï¸âƒ£ ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸° (ì—…ë°ì´íŠ¸ ì €ì¥)
final_df.to_excel(market_data_path, index=False)

print(f"[OK] market_data.xlsx íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤: {market_data_path}")


# -*- coding: utf-8 -*-
"""
í´ë” ë¹„ìš°ê¸° ìœ í‹¸ (ë³´ì¡´ ì˜ˆì™¸ ì§€ì›, **í´ë”ëŠ” ì ˆëŒ€ ì‚­ì œí•˜ì§€ ì•ŠìŒ**)
- target í´ë” ë‚´ë¶€ì˜ íŒŒì¼/í•˜ìœ„í´ë” 'ë‚´ìš©'ë§Œ ì‚­ì œí•˜ë˜, ì§€ì •í•œ ì˜ˆì™¸ íŒŒì¼/í´ë”ëŠ” ë³´ì¡´
- dry_run=True ì‹œ ì‹¤ì œ ì‚­ì œ ì—†ì´ ë¡œê·¸ë§Œ ì¶œë ¥
- Windowsì—ì„œë„ ì•ˆì „í•˜ê²Œ: pathlib.Path ì‚¬ìš© + / ìŠ¬ë˜ì‹œ ê²½ë¡œ
"""

from pathlib import Path
import shutil  # (ë‚¨ê²¨ë‘ : ì¶”í›„ í™•ì¥ ëŒ€ë¹„, í˜„ì¬ í´ë” ì‚­ì œì—ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
from typing import Iterable, Set

# [ì„¤ì •] ëŒ€ìƒ í´ë”ë“¤
TARGET_BIGKINDS = Path(r"C:/Users/amongpapa/chartup/go_scen/data/news/bigkinds")
TARGET_DAILY_NEWS = Path(r"C:/Users/amongpapa/chartup/go_scen/data/news/bigkinds/daily_news")

def _is_dangerous_path(p: Path) -> bool:
    """
    ë£¨íŠ¸ ê°™ì€ ìœ„í—˜ ê²½ë¡œë¥¼ ë³´í˜¸í•˜ê¸° ìœ„í•œ ê°„ë‹¨ ê°€ë“œ.
    - ë“œë¼ì´ë¸Œ ë£¨íŠ¸(ex: C:/) ë˜ëŠ” ê²½ë¡œ ë¬¸ìì—´ì´ ë„ˆë¬´ ì§§ì€ ê²½ìš° ìœ„í—˜ìœ¼ë¡œ ê°„ì£¼
    """
    try:
        p = p.resolve()
    except Exception:
        return True
    return (p == Path(p.anchor)) or (len(str(p)) < 10)

def purge_dir(target: Path, dry_run: bool = False, exclude: Iterable[Path] = ()) -> None:
    """
    target í´ë” ì•ˆì˜ ëª¨ë“  'ë‚´ìš©ë¬¼'ì„ ì •ë¦¬(íŒŒì¼/ë§í¬ ì‚­ì œ)í•˜ë˜, **í´ë” ìì²´ëŠ” ì‚­ì œí•˜ì§€ ì•ŠìŒ**.
    excludeëŠ” ì ˆëŒ€/ìƒëŒ€ ê²½ë¡œ ëª¨ë‘ í—ˆìš©:
      - ì ˆëŒ€ê²½ë¡œ: ê·¸ëŒ€ë¡œ ë³´ì¡´
      - ìƒëŒ€ê²½ë¡œ: target ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œë¡œ í•´ì„í•˜ì—¬ ë³´ì¡´
    """
    target = Path(target).resolve()
    if not target.is_dir():
        raise FileNotFoundError(f"ëŒ€ìƒ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {target}")

    if _is_dangerous_path(target):
        raise ValueError(f"ìœ„í—˜í•œ ê²½ë¡œë¡œ íŒë‹¨ë˜ì–´ ì¤‘ë‹¨í•©ë‹ˆë‹¤: {target}")

    # ë³´ì¡´(ì œì™¸) ì§‘í•©: ì ˆëŒ€ ê²½ë¡œë¡œ ì •ê·œí™”
    exclude_abs: Set[Path] = set()
    for ex in exclude:
        ex = Path(ex)
        if not ex.is_absolute():
            ex = (target / ex).resolve()
        else:
            ex = ex.resolve()
        exclude_abs.add(ex)

    def _purge(curr: Path) -> None:
        for child in curr.iterdir():
            try:
                cres = child.resolve()

                # ----------------------------
                # 1) í´ë” ìì²´ê°€ excludeë©´: í†µì§¸ë¡œ ê±´ë“œë¦¬ì§€ ì•ŠìŒ
                # ----------------------------
                if cres in exclude_abs and child.is_dir():
                    print(f"[SKIP] ì œì™¸ í´ë” ë³´ì¡´: {child}")
                    continue

                # ----------------------------
                # 2) íŒŒì¼/ë§í¬ ì²˜ë¦¬
                # ----------------------------
                if child.is_file() or child.is_symlink():
                    if cres in exclude_abs:
                        print(f"[SKIP] ì œì™¸ íŒŒì¼ ë³´ì¡´: {child}")
                        continue
                    if dry_run:
                        print(f"[DRY ] íŒŒì¼ ì‚­ì œ ì˜ˆì •: {child}")
                    else:
                        child.unlink()
                        print(f"[DEL ] íŒŒì¼ ì‚­ì œ: {child}")
                    continue

                # ----------------------------
                # 3) í´ë” ì²˜ë¦¬ (ì¤‘ìš”)
                #    - **í´ë”ëŠ” ì ˆëŒ€ ì‚­ì œí•˜ì§€ ì•ŠìŒ**
                #    - ë‚´ë¶€ë¡œ ì¬ê·€ ì§„ì…í•˜ì—¬ íŒŒì¼ë§Œ ì •ë¦¬
                # ----------------------------
                if child.is_dir():
                    _purge(child)
                    # í´ë” ì‚­ì œ ë¡œì§ì€ ì „ë¶€ ì œê±° (í´ë” êµ¬ì¡°ëŠ” í•­ìƒ ìœ ì§€)
            except Exception as e:
                print(f"[ERR ] ì‚­ì œ ì‹¤íŒ¨: {child} -> {e}")

    _purge(target)


if __name__ == "__main__":
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1) daily_news í´ë”: keyword_news.xlsxëŠ” ë³´ì¡´(ì‚­ì œ ì œì™¸)
    #    â†’ ìƒëŒ€ê²½ë¡œ 'keyword_news.xlsx'ëŠ” TARGET_DAILY_NEWS ê¸°ì¤€ìœ¼ë¡œ í•´ì„ë¨
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    EXCLUDE_IN_DAILY = [Path("keyword_news.xlsx")]

    # [ë³€ê²½] 2) bigkinds ì „ì²´ ì •ë¦¬ ì‹œì—ë„ keyword_news.xlsx ë³´ì¡´
    # - ì—¬ê¸°ì„œëŠ” ì ˆëŒ€ê²½ë¡œë¡œ ì˜ˆì™¸ë¥¼ ë„£ì–´ì¤Œ
    # - TARGET_BIGKINDS ì•„ë˜ ì–´ë””ë¥¼ ëŒë”ë¼ë„ ì´ íŒŒì¼ì€ ì‚­ì œë˜ì§€ ì•Šê²Œ ë³´í˜¸
    EXCLUDE_IN_BIGKINDS = [
        TARGET_DAILY_NEWS / "keyword_news.xlsx"   # [ë³€ê²½] ì ˆëŒ€ê²½ë¡œ ì˜ˆì™¸ ì§€ì •
    ]

    # (ìƒ˜í”Œ) ì˜ˆí–‰ì—°ìŠµ: ë¬´ì—‡ì´ ì§€ì›Œì§ˆì§€ ë¡œê·¸ë§Œ í™•ì¸í•  ë•Œ ì‚¬ìš©
    # purge_dir(TARGET_DAILY_NEWS, dry_run=True,  exclude=EXCLUDE_IN_DAILY)
    # purge_dir(TARGET_BIGKINDS,   dry_run=True,  exclude=EXCLUDE_IN_BIGKINDS)

    # ì‹¤ì œ ì‹¤í–‰: **í´ë”ëŠ” ë‚¨ê¸°ê³  íŒŒì¼/ë§í¬ë§Œ ì‚­ì œ**
    purge_dir(TARGET_DAILY_NEWS, dry_run=False, exclude=EXCLUDE_IN_DAILY)
    purge_dir(TARGET_BIGKINDS,   dry_run=False, exclude=EXCLUDE_IN_BIGKINDS)  # [ë³€ê²½]


from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
import os
import time
import datetime
import shutil

# ğŸ“Œ ë‹¤ìš´ë¡œë“œ í´ë” ì„¤ì •
download_folder = r"C:\Users\amongpapa\chartup\go_scen\data\news\bigkinds"

# í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
if not os.path.exists(download_folder):
    os.makedirs(download_folder)

# ğŸ”¹ ì˜¤ëŠ˜ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸° (YYYYMMDD í˜•ì‹)
today_date = datetime.datetime.today().strftime("%Y%m%d")

# ğŸ”¹ í¬ë¡¬ ì˜µì…˜ ì„¤ì • (ìë™ ë‹¤ìš´ë¡œë“œ í™œì„±í™”)
chrome_options = webdriver.ChromeOptions()
prefs = {
    "download.default_directory": download_folder,
    "download.prompt_for_download": False,
    "download.directory_upgrade": True,
    "safebrowsing.enabled": True
}
chrome_options.add_experimental_option("prefs", prefs)

# ğŸ”¹ í¬ë¡¬ ë“œë¼ì´ë²„ ì‹¤í–‰
driver = webdriver.Chrome(options=chrome_options)

# ğŸ”¹ ì‚¬ì´íŠ¸ ì ‘ì†
driver.get("https://www.bigkinds.or.kr/v2/mypage/myKeyword.do")

# ğŸ”¹ ë¡œê·¸ì¸ ë²„íŠ¼ì— ë§ˆìš°ìŠ¤ ì˜¤ë²„í•˜ì—¬ ì‘ì€ íŒì—… í‘œì‹œ
time.sleep(2)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
login_menu = driver.find_element(By.CSS_SELECTOR, "#header > div.header-anim > div > div > div.topRightArea > ul > li.topMembership > a")
ActionChains(driver).move_to_element(login_menu).perform()

# ğŸ”¹ ì‘ì€ íŒì—…ì—ì„œ ë¡œê·¸ì¸ ë§í¬ í´ë¦­
time.sleep(1)  # íŒì—… í‘œì‹œ ëŒ€ê¸°
login_link = driver.find_element(By.XPATH, '//*[@id="header"]/div[2]/div/div/div[2]/ul/li[1]/div/ul/li/a')
login_link.click()

# ğŸ”¹ ë¡œê·¸ì¸ íŒì—…ì—ì„œ ì•„ì´ë””ì™€ ë¹„ë°€ë²ˆí˜¸ ì…ë ¥
time.sleep(2)  # ë¡œê·¸ì¸ íŒì—… ë¡œë”© ëŒ€ê¸°
username = "amongddomong@gmail.com"
password = "^^joahane1"

# ì•„ì´ë”” ì…ë ¥
id_input = driver.find_element(By.XPATH, '//*[@id="login-user-id"]')
id_input.clear()  # ğŸ‘‰ ê¸°ì¡´ ì…ë ¥ê°’ ì œê±°
id_input.send_keys(username)

# ë¹„ë°€ë²ˆí˜¸ ì…ë ¥
pw_input = driver.find_element(By.XPATH, '//*[@id="login-user-password"]')
pw_input.clear()  # ğŸ‘‰ ê¸°ì¡´ ì…ë ¥ê°’ ì œê±°
pw_input.send_keys(password)

# ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­
login_button = driver.find_element(By.XPATH, '//*[@id="login-btn"]')
login_button.click()

# ğŸ”¹ ë¡œê·¸ì¸ í›„ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
time.sleep(5)

# ğŸ”¹ ë‹¤ìš´ë¡œë“œ í˜ì´ì§€ë¡œ ì§ì ‘ ì´ë™
driver.get("https://www.bigkinds.or.kr/v2/mypage/myKeyword.do")

# ğŸ”¹ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ í´ë¦­ (JS ê°•ì œ í´ë¦­ ë°©ì‹)
time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
download_button = driver.find_element(By.XPATH, '//*[@id="btn-keyword-download"]')
driver.execute_script("arguments[0].click();", download_button)

# ğŸ”¹ íŒì—…ì´ ë‚˜íƒ€ë‚˜ë©´ "í™•ì¸" ë²„íŠ¼ í´ë¦­
time.sleep(3)
try:
    alert = driver.switch_to.alert
    print("íŒì—… ê°ì§€ë¨! í™•ì¸ ë²„íŠ¼ í´ë¦­ ì¤‘...")
    alert.accept()
    print("í™•ì¸ ë²„íŠ¼ í´ë¦­ ì™„ë£Œ!")
except:
    print("íŒì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ.")

# ğŸ”¹ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°
time.sleep(50)

# ğŸ”¹ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ì´ë¦„ ë³€ê²½ (ê°€ì¥ ìµœê·¼ íŒŒì¼ ì°¾ê¸°)
downloaded_files = os.listdir(download_folder)
if downloaded_files:
    downloaded_files = sorted(
        downloaded_files,
        key=lambda x: os.path.getctime(os.path.join(download_folder, x)),
        reverse=True
    )
    latest_file = os.path.join(download_folder, downloaded_files[0])
    new_filename = os.path.join(download_folder, f"{today_date}_bigkinds.xlsx")

    shutil.move(latest_file, new_filename)
    print(f"íŒŒì¼ëª…ì´ '{new_filename}'(ìœ¼)ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤!")

print(f"íŒŒì¼ì´ {download_folder} í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

# ğŸ”¹ ë“œë¼ì´ë²„ ì¢…ë£Œ
driver.quit()


import pandas as pd
import os
from datetime import datetime, timedelta

# ğŸ”¹ ì˜¤ëŠ˜ ë‚ ì§œ ë¬¸ìì—´ ìƒì„± (YYYYMMDD)
today = datetime.now()
today_str = today.strftime("%Y%m%d")

# ğŸ”¹ í•˜ë£¨ ì „ ë‚ ì§œ ë¬¸ìì—´ ìƒì„±
yesterday = today - timedelta(days=1)
yesterday_str = yesterday.strftime("%Y%m%d")

# ğŸ”¹ ì›ë³¸ íŒŒì¼ ê²½ë¡œ (ì˜¤ëŠ˜ ê¸°ì¤€)
input_path = rf"C:\Users\amongpapa\chartup\go_scen\data\news\bigkinds\{today_str}_bigkinds.xlsx"

# ğŸ”¹ ê²°ê³¼ ì €ì¥ íŒŒì¼ ê²½ë¡œ
output_path = rf"C:\Users\amongpapa\chartup\go_scen\data\news\bigkinds\{today_str}_bigkinds_risk.xlsx"

# ğŸ”¹ ì—‘ì…€ íŒŒì¼ ë¡œë“œ
df = pd.read_excel(input_path)

# ğŸ”¹ 'ì¼ì' ì»¬ëŸ¼ì´ datetimeì´ ì•„ë‹ˆë¼ë©´ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
df["ì¼ì"] = df["ì¼ì"].astype(str)

# ğŸ”¹ ì˜¤ëŠ˜ê³¼ í•˜ë£¨ ì „ ë‚ ì§œ í¬í•¨í•˜ëŠ” ë°ì´í„°ë§Œ í•„í„°ë§
df = df[df["ì¼ì"].isin([today_str, yesterday_str])]

# ğŸ”¹ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
df = df[['ì¼ì', 'ì œëª©', 'ì–¸ë¡ ì‚¬', 'í†µí•© ë¶„ë¥˜1', 'URL']]

# ğŸ”¹ ì œì™¸í•  í†µí•© ë¶„ë¥˜1 ì•ê¸€ì ë¦¬ìŠ¤íŠ¸
excluded_prefixes = ['IT_ê³¼í•™', 'ë¬¸í™”', 'ë¯¸ë¶„ë¥˜', 'ìŠ¤í¬ì¸ ']

# ğŸ”¹ ì œì™¸ ì¡°ê±´ ì ìš©
df = df[~df["í†µí•© ë¶„ë¥˜1"].str.startswith(tuple(excluded_prefixes), na=False)]

# ğŸ”¹ URL ê°’ì´ NULLì¸ í–‰ ì œê±°
df = df[df["URL"].notna()]

# ğŸ”¹ ê²°ê³¼ ì €ì¥
df.to_excel(output_path, index=False)
print(f"âœ… ì˜¤ëŠ˜ê³¼ í•˜ë£¨ ì „ ë‚ ì§œ ê¸°ì¤€ í•„í„°ë§ëœ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}")

# ğŸ”¹ ì›ë³¸ íŒŒì¼ ì‚­ì œ
if os.path.exists(input_path):
    os.remove(input_path)
    print(f"ğŸ—‘ï¸ ì›ë³¸ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {input_path}")
else:
    print(f"âš ï¸ ì›ë³¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_path}")


import pandas as pd
import datetime as dt
import os
import re
import glob
from typing import List, Optional


def _pick_col(df: pd.DataFrame, candidates: List[str], required: bool = True) -> Optional[str]:
    """
    ë‹¤ì–‘í•œ í‘œê¸°(ëŒ€ì†Œë¬¸ì/í•œì˜í˜¼ìš©/ìŠ¤í˜ì´ìŠ¤ ìœ ë¬´)ë¥¼ ê³ ë ¤í•´ ê°€ì¥ ë¨¼ì € ë§¤ì¹­ë˜ëŠ” ì»¬ëŸ¼ëª…ì„ ì°¾ì•„ ë°˜í™˜.
    required=Falseì´ë©´ ëª» ì°¾ì„ ê²½ìš° None ë°˜í™˜.
    """
    norm = {re.sub(r"\s+", "", str(c)).lower(): c for c in df.columns}
    for cand in candidates:
        key = re.sub(r"\s+", "", cand).lower()
        if key in norm:
            return norm[key]
    if required:
        raise KeyError(f"í•„ìˆ˜ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í›„ë³´: {candidates}")
    return None


def _load_keywords(keyword_path: str) -> pd.DataFrame:
    """
    keyword.xlsx ë¡œë“œ ë° í‘œì¤€ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì •ë¦¬
    ê¸°ëŒ€ ì»¬ëŸ¼(ëŒ€ì²´ í—ˆìš©): 
      - Scenario_ID: ['Scenario_ID','ì‹œë‚˜ë¦¬ì˜¤ID','ì‹œë‚˜ë¦¬ì˜¤','ScenarioId','SCENARIO_ID']
      - News_kor   : ['News_kor','news_kor','News_KOR','í‚¤ì›Œë“œ','Keyword']
      - Phase      : ['Phase','phase','ë‹¨ê³„']
    """
    df = pd.read_excel(keyword_path)
    sc_col = _pick_col(df, ["Scenario_ID", "ì‹œë‚˜ë¦¬ì˜¤ID", "ì‹œë‚˜ë¦¬ì˜¤", "ScenarioId", "SCENARIO_ID"])
    kw_col = _pick_col(df, ["News_kor", "news_kor", "News_KOR", "í‚¤ì›Œë“œ", "Keyword"])
    ph_col = _pick_col(df, ["Phase", "phase", "ë‹¨ê³„"], required=False)

    out = pd.DataFrame({
        "Scenario_ID": df[sc_col].astype(str).str.strip(),
        "News_kor": df[kw_col].astype(str).str.strip(),
        "Phase": df[ph_col].astype(str).str.strip() if ph_col else ""
    })
    # ë¹ˆ í‚¤ì›Œë“œ ì œê±°
    out = out[out["News_kor"].str.len() > 0].reset_index(drop=True)
    return out


def _load_latest_bigkinds(risk_dir: str) -> pd.DataFrame:
    """
    risk_dir ë‚´ '*bigkinds_risk*.xlsx' íŒŒì¼ ì¤‘ **ê°€ì¥ ìµœì‹  ìˆ˜ì •ì‹œê°** íŒŒì¼ì„ ì½ì–´ í‘œì¤€ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë°˜í™˜
    ê¸°ëŒ€ ì»¬ëŸ¼(ëŒ€ì²´ í—ˆìš©):
      - ì œëª©   : ['ì œëª©','ê¸°ì‚¬ì œëª©','title','Title']
      - ì–¸ë¡ ì‚¬ : ['ì–¸ë¡ ì‚¬','ì–¸ë¡ ì‚¬ëª…','ë§¤ì²´','publisher','Publisher','ì‹ ë¬¸ì‚¬']
      - URL   : ['URL','url','ë§í¬','Link']
    """
    paths = glob.glob(os.path.join(risk_dir, "*bigkinds_risk*.xlsx"))
    if not paths:
        raise FileNotFoundError(f"bigkinds_risk íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {risk_dir}")

    latest = max(paths, key=os.path.getmtime)  # [ë³€ê²½ì ] ê°€ì¥ ìµœê·¼ íŒŒì¼ ìë™ ì„ íƒ
    df = pd.read_excel(latest)

    title_col = _pick_col(df, ["ì œëª©", "ê¸°ì‚¬ì œëª©", "title", "Title"])
    media_col = _pick_col(df, ["ì–¸ë¡ ì‚¬", "ì–¸ë¡ ì‚¬ëª…", "ë§¤ì²´", "publisher", "Publisher", "ì‹ ë¬¸ì‚¬"])
    url_col   = _pick_col(df, ["URL", "url", "ë§í¬", "Link"])

    out = pd.DataFrame({
        "ì œëª©": df[title_col].astype(str).str.strip(),
        "ì–¸ë¡ ì‚¬": df[media_col].astype(str).str.strip(),
        "URL": df[url_col].astype(str).str.strip()
    }).dropna(subset=["ì œëª©"]).reset_index(drop=True)

    # ì¤‘ë³µ ì œëª© ì œê±°(ì„ í–‰ ë°ì´í„° ìš°ì„ )
    out = out.drop_duplicates(subset=["ì œëª©"], keep="first").reset_index(drop=True)
    return out


def _match_first(corpus: pd.DataFrame, query: str, used_titles: set) -> Optional[pd.Series]:
    """
    News_kor ë¬¸ìì—´ì„ ',' ê¸°ì¤€ **AND ì¡°ê±´**ìœ¼ë¡œ ë¶„í•´í•˜ì—¬ ê¸°ì‚¬ ì œëª©ì—ì„œ ëª¨ë‘ ë§¤ì¹­ë˜ëŠ” ì²« ê±´ ë°˜í™˜.
    ì´ë¯¸ ì„ íƒëœ ì œëª©(used_titles)ì€ ì œì™¸.
    """
    # 'í‚¤ì›Œë“œ1,í‚¤ì›Œë“œ2' -> ['í‚¤ì›Œë“œ1','í‚¤ì›Œë“œ2']
    terms = [t.strip() for t in str(query).split(",") if t.strip()]
    mask = pd.Series(True, index=corpus.index)
    for t in terms:
        # ëŒ€ì†Œë¬¸ì ë¬´ì‹œ, ì •ê·œì‹ ë©”íƒ€ë¬¸ì ì´ìŠ¤ì¼€ì´í”„
        mask &= corpus["ì œëª©"].str.contains(re.escape(t), case=False, na=False)

    candidates = corpus[mask & ~corpus["ì œëª©"].isin(used_titles)]
    if candidates.empty:
        return None
    # ì²« ê±´ ì„ íƒ(ê°€ì¥ ë‹¨ìˆœí•˜ê³  ì˜ˆì¸¡ê°€ëŠ¥)  [ë³€ê²½ì ] ìš°ì„ ìˆœìœ„ì–¸ë¡ ì‚¬ ê·œì¹™ ì œê±°
    return candidates.iloc[0]


def extract_daily_news_from_keyword_file(
    keyword_path: str,
    output_dir: str,
    risk_dir: str = r"C:\Users\amongpapa\uto\risk_report\bigkinds"
):
    """
    [ë³€ê²½ì ] ìš”êµ¬ì‚¬í•­ì— ë§ì¶˜ ìƒˆë¡œìš´ ë©”ì¸ í•¨ìˆ˜
      - ì…ë ¥: keyword.xlsx (Scenario_ID, News_kor, Phase)
      - ë¹…ì¹´ì¸ì¦ˆ: risk_dirì˜ ìµœì‹  '*bigkinds_risk*.xlsx'
      - ë§¤ì¹­ ë¡œì§: News_korë¥¼ ','ë¡œ ë¶„í•´í•œ **AND ì¡°ê±´**ìœ¼ë¡œ ì œëª© ê²€ìƒ‰, í‚¤ì›Œë“œë‹¹ 1ê±´ ì„ ì •(ì¤‘ë³µ ì œëª©ì€ ìŠ¤í‚µ)
      - ì¶œë ¥ ì»¬ëŸ¼: Scenario_ID, News_kor, Phase, ì œëª©, ì–¸ë¡ ì‚¬, URL
      - íŒŒì¼ëª…: YYYYMMDD_news.xlsx
    """
    # 1) í‚¤ì›Œë“œ ë¡œë“œ
    df_kw = _load_keywords(keyword_path)

    # 2) ìµœì‹  bigkinds ë°ì´í„° ë¡œë“œ
    df_risk = _load_latest_bigkinds(risk_dir)

    # 3) ë§¤ì¹­ ìˆ˜í–‰
    used_titles = set()
    rows = []
    not_found = []

    for _, r in df_kw.iterrows():
        scenario_id = r["Scenario_ID"]
        news_kor = r["News_kor"]
        phase = r["Phase"]

        hit = _match_first(df_risk, news_kor, used_titles)
        if hit is None:
            not_found.append((scenario_id, news_kor))
            continue

        used_titles.add(hit["ì œëª©"])
        rows.append({
            "Scenario_ID": scenario_id,
            "News_kor": news_kor,
            "Phase": phase,
            "ì œëª©": hit["ì œëª©"],
            "ì–¸ë¡ ì‚¬": hit["ì–¸ë¡ ì‚¬"],
            "URL": hit["URL"],
        })

    # 4) ì €ì¥
    os.makedirs(output_dir, exist_ok=True)
    date_str = dt.datetime.now().strftime("%Y%m%d")
    out_file = os.path.join(output_dir, f"keyword_news_bf.xlsx")
    pd.DataFrame(rows).to_excel(out_file, index=False)

    print(f"[ì™„ë£Œ] ë°ì¼ë¦¬ ê¸°ì‚¬ íŒŒì¼: {out_file}")
    print(f" - ë§¤ì¹­ ì„±ê³µ {len(rows)}ê±´ / ì‹¤íŒ¨ {len(not_found)}ê±´")
    if not_found:
        print(" - ë§¤ì¹­ ì‹¤íŒ¨ ë¦¬ìŠ¤íŠ¸(Scenario_ID, News_kor):")
        for sc, kw in not_found[:20]:  # ë„ˆë¬´ ê¸¸ë©´ ì• 20ê°œë§Œ
            print(f"   Â· {sc} | {kw}")


if __name__ == "__main__":
    # âš ï¸ ê²½ë¡œëŠ” r"..." raw stringìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš” (ìœˆë„ìš° ë°±ìŠ¬ë˜ì‹œ ì´ìŠ¤ì¼€ì´í”„ ë°©ì§€)
    keyword_path = r"C:\Users\amongpapa\chartup\go_scen\data\news\keyword.xlsx"   # [ë³€ê²½ì ]
    output_dir   = r"C:\Users\amongpapa\chartup\go_scen\data\news\bigkinds\daily_news"                          # í•„ìš”ì— ë§ê²Œ ì¡°ì •
    risk_dir     = r"C:\Users\amongpapa\chartup\go_scen\data\news\bigkinds"                 # í•„ìš” ì‹œ ì¡°ì •
    extract_daily_news_from_keyword_file(keyword_path, output_dir, risk_dir)


# -*- coding: utf-8 -*-
r"""
ëª©ì (ì—…ë°ì´íŠ¸ ë²„ì „ - ìš´ì˜ìš©)
- (ì†ŒìŠ¤) daily_news í´ë”ì˜ 'keyword_news_bf.xlsx' ë§Œ ì²˜ë¦¬
- ê° í–‰ì˜ URL í˜ì´ì§€ë¥¼ Seleniumìœ¼ë¡œ ì—´ì–´ body.innerText ì¶”ì¶œ â†’ GPT ìš”ì•½ ìƒì„±
- (íƒ€ê¹ƒ) 'keyword_news.xlsx'ëŠ” ì €ì¥ìš©: ê¸°ì¡´ ë‚´ìš©ì€ ìœ ì§€í•˜ë©´ì„œ
  - ì²˜ë¦¬ëœ í–‰ë“¤ì„ 'ê·¸ëŒ€ë¡œ' ì•„ë˜ë¡œ ì´ì–´ì„œ ì¶”ê°€(append)
  - ì „ì²´ í–‰ ìˆ˜ê°€ max_rows(ê¸°ë³¸ 30)ë¥¼ ë„˜ìœ¼ë©´, 'ë¨¼ì € ë“¤ì–´ì˜¨ í–‰'ë¶€í„° ì œê±°(FIFO)í•˜ì—¬ ìµœê·¼ max_rowsí–‰ ìœ ì§€

íŠ¹ì§•
- news ì»¬ëŸ¼ ë¹„ì–´ ìˆëŠ” í–‰ë§Œ ì²˜ë¦¬(FILL_ONLY_EMPTY=True)
- íŠ¹ì • ì–¸ë¡ ì‚¬ì—ì„œ timeout ë‚˜ë”ë¼ë„ ê·¸ í–‰ë§Œ ìŠ¤í‚µí•˜ê³  ë‹¤ìŒ í–‰ ì²˜ë¦¬
- Windows ê²½ë¡œëŠ” pathlib.Path(r"...") ì‚¬ìš©
"""

import re
import time
from typing import List, Optional
from pathlib import Path

import pandas as pd

# âœ… Selenium
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.common.exceptions import WebDriverException, TimeoutException, NoSuchElementException

# =========================
# ê²½ë¡œ ë° ì˜µì…˜
# =========================
DIR_PATH = Path(r"C:\Users\amongpapa\chartup\go_scen\data\news\bigkinds\daily_news")  # ì‘ì—… í´ë”
KEY_PATH = Path(r"C:\Users\amongpapa\lm\keys\open.txt")  # OpenAI í‚¤ íŒŒì¼
SOURCE_BF = DIR_PATH / "keyword_news_bf.xlsx"  # ì†ŒìŠ¤(ë§¤ì¼ ê°±ì‹ )
TARGET_NEWS = DIR_PATH / "keyword_news.xlsx"   # íƒ€ê¹ƒ(ì €ì¥/ë¡¤ë§)

# ğŸ” ìˆ˜ì •: ìµœëŒ€ í–‰ ìˆ˜ 20 â†’ 30 (ìµœê·¼ 30ê°œë§Œ ìœ ì§€)
MAX_TARGET_ROWS = 30  # íƒ€ê¹ƒ ìµœëŒ€ í–‰ ìˆ˜(ë„˜ì¹˜ë©´ FIFO, ìµœê·¼ 30ê°œ ìœ ì§€)

# âœ… daily_news í´ë”ê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
DIR_PATH.mkdir(parents=True, exist_ok=True)

# íŒ€ì¥ë‹˜ í™˜ê²½
CHROME_PATH = Path(r"C:\Program Files\Google\Chrome\Application\chrome.exe")
CHROME_PROFILE = "Profile 1"

MODEL_ID = "gpt-4o-mini"
FILL_ONLY_EMPTY = True
TARGET_NEWS_COL_INDEX = 6  # Gì—´(0-based=6)
PAGE_LOAD_WAIT = 8.0       # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°(ì´ˆ)
REQUEST_INTERVAL_SEC = 0.4 # GPT í˜¸ì¶œ ê°„ê²©

# =========================
# OpenAI ì‹ /êµ¬ SDK ìë™ í˜¸í™˜
# =========================
def load_api_key(path: Path) -> str:
    """open.txtì—ì„œ OpenAI API í‚¤ë¥¼ í•œ ì¤„ ì½ì–´ì˜¤ëŠ” í•¨ìˆ˜."""
    key = path.read_text(encoding="utf-8").strip()
    if not key:
        raise ValueError("API í‚¤ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    return key


class ChatClient:
    """ì‹ /êµ¬ OpenAI SDK ìë™ ê°ì§€ ë˜í¼(.chat -> str)."""
    def __init__(self, api_key: str):
        self.mode = None
        try:
            from openai import OpenAI  # ì‹ ë²„ì „
            self._client = OpenAI(api_key=api_key)
            self.mode = "new"
        except Exception:
            import openai  # êµ¬ë²„ì „
            openai.api_key = api_key
            self._client = openai
            self.mode = "old"

    def chat(self, messages: List[dict], model: str, temperature: float = 0.2) -> str:
        """messages ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ GPTë¡œë¶€í„° ë¬¸ìì—´ ë‹µë³€ë§Œ ë°˜í™˜í•˜ëŠ” ê³µí†µ ì¸í„°í˜ì´ìŠ¤."""
        if self.mode == "new":
            res = self._client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
            )
            return res.choices[0].message.content.strip()
        else:
            res = self._client.ChatCompletion.create(
                model=model,
                messages=messages,
                temperature=temperature,
            )
            return res["choices"][0]["message"]["content"].strip()


# =========================
# í…ìŠ¤íŠ¸ ì •ë¦¬ & í”„ë¡¬í”„íŠ¸ êµ¬ì„±
# =========================
PROMPT_TEMPLATE = (
    "ê¸°ì‚¬ ìš”ì•½ â€“ ì„œìˆ í˜• ë³´ê³ ì„œ\n"
    "[ì—­í• ] ë‹¹ì‹ ì€ ì€í–‰ ë¦¬ìŠ¤í¬ê´€ë¦¬ ë³´ê³ ì„œ ì—ë””í„°ë‹¤.\n"
    "[ëª©í‘œ] ì—¬ëŸ¬ ê¸°ì‚¬ ì›ë¬¸ì—ì„œ ê´‘ê³ /í™ë³´/ë¬´ê´€/ì¤‘ë³µ ë‚´ìš©ì„ ì œê±°í•˜ê³  í•µì‹¬ ì‚¬ì‹¤ë§Œì„ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ ì„œìˆ í˜• ë³´ê³ ì„œë¥¼ ì‘ì„±í•œë‹¤. "
    "ì´ ê¸¸ì´ {2000}ì ì´ë‚´.\n"
    "[ì…ë ¥] {ê¸°ì‚¬ ì›ë¬¸ë“¤}\n"
    "[ì¶œë ¥ ì§€ì¹¨ â€“ ì„œìˆ í˜• ë³´ê³ ì„œ]\n"
    "- ì œëª©: í•œ ì¤„\n"
    "- ìš”ì•½: 3ë¬¸ì¥\n"
    "- ë³¸ë¬¸: ì—°ëŒ€ê¸° ìˆœ(ì‚¬ì‹¤â†’ì›ì¸â†’ì˜í–¥â†’ì „ë§)ìœ¼ë¡œ 3â€“6ë‹¨ë½. ìˆ«ìÂ·ê¸°ê´€ëª…Â·ì§€ëª… ì •í™•íˆ ê¸°ìˆ .\n"
    "- í•µì‹¬ ìˆ˜ì¹˜: ë³¸ë¬¸ì— ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ ì“°ë˜ ìµœì´ˆ ë“±ì¥ ë¬¸ì¥ ëì— ê·¼ê±° [s#] í‘œê¸°(ìµœëŒ€ 5ê°œ).\n"
    "- ìì‚°/ì„¹í„°/í‹°ì»¤: í•„ìš”í•œ ê²½ìš° ê´„í˜¸ë¡œ ê°„ë‹¨ í‘œê¸°(ì˜ˆ: ì›/ë‹¬ëŸ¬ í™˜ìœ¨(USDKRW), ì‚¼ì„±ì „ì(005930.KS)).\n"
    "- ì‹œë‚˜ë¦¬ì˜¤ ì—°ê²°: ë¬¸ë‹¨ ë§ë¯¸ì— 1â€“2ë¬¸ì¥ìœ¼ë¡œ â€˜ì£¼ìš” ë™ì¸Â·ì§€í‘œÂ·íŠ¸ë¦¬ê±°Â·ë°©í–¥ì„±Â·ì˜ˆìƒ ì‹œê³„â€™ë¥¼ í†µí•© ì„œìˆ .\n"
    "- ì‹ ë¢°ë„: ë§ˆì§€ë§‰ ë¬¸ì¥ì— (ì‹ ë¢°ë„ 0.0â€“1.0; ê·¼ê±° ìš”ì•½) í˜•ì‹ìœ¼ë¡œ í‘œê¸°.\n"
    "- ì¤‘ë³µ/ì œê±°: ì‚­ì œÂ·í†µí•©í•œ ê¸°ì‚¬ ì œëª©(ë˜ëŠ” ìš”ì§€)ë§Œ ë§ˆì§€ë§‰ ì¤„ì— â€˜ì œê±°: â€¦â€™ë¡œ ë‚˜ì—´.\n"
    "[ê¸ˆì§€] ê´‘ê³ ë¬¸êµ¬, ê³¼ì¥/ì¶”ì¸¡, ë¶ˆí•„ìš” í…ìŠ¤íŠ¸, í‘œ/ë¶ˆë¦¿/JSON ì¶œë ¥.\n"
    "[ì‹¤í–‰] ìœ„ ì§€ì¹¨ì„ ì ìš©í•´ ê²°ê³¼ë§Œ ì¶œë ¥í•œë‹¤.\n"
)


def clean_text(text: str) -> str:
    """
    ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì •ì œ:
    - ê°œí–‰/ê³µë°± ì •ë¦¬
    - ì¿ í‚¤/ì €ì‘ê¶Œ/êµ¬ë…/ê´‘ê³  ê´€ë ¨ ë¬¸êµ¬ ì œê±°
    """
    text = re.sub(r"\r", "\n", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    text = re.sub(r"[ \t]+", " ", text)

    drop_patterns = [
        r"ì¿ í‚¤(ë¥¼|ì—) ì‚¬ìš©",
        r"ì´ìš©ì•½ê´€",
        r"ê°œì¸ì •ë³´",
        r"êµ¬ë…",
        r"ê´‘ê³ ë¬¸ì˜",
        r"ë¬´ë‹¨ì „ì¬",
        r"ì €ì‘ê¶Œ",
    ]

    cleaned_lines = []
    for ln in text.splitlines():
        if any(re.search(pat, ln, re.IGNORECASE) for pat in drop_patterns):
            continue
        cleaned_lines.append(ln.strip())
    return "\n".join(cleaned_lines).strip()


def build_prompt(article_text: str, max_chars: int = 2000) -> str:
    """
    ê¸°ì‚¬ í…ìŠ¤íŠ¸ë¥¼ PROMPT_TEMPLATEì— ì±„ì›Œ ë„£ì–´ GPTìš© í”„ë¡¬í”„íŠ¸ ìƒì„±.
    - ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” 8,000ìê¹Œì§€ë§Œ ì‚¬ìš©
    """
    article_text = article_text[:8000]
    return (
        PROMPT_TEMPLATE
        .replace("{2000}", str(max_chars))
        .replace("{ê¸°ì‚¬ ì›ë¬¸ë“¤}", article_text)
    )


def summarize_article(client: ChatClient, article_text: str, model: str = MODEL_ID) -> str:
    """
    ì •ì œëœ ê¸°ì‚¬ í…ìŠ¤íŠ¸ â†’ GPT ìš”ì•½.
    - ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¡œ 'ë¦¬ìŠ¤í¬ê´€ë¦¬ ë³´ì¡°ì›' ì—­í•  ì§€ì •
    - ì¶œë ¥ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ ì„œìˆ í˜•
    """
    sys_msg = (
        "ë„ˆëŠ” ì€í–‰ ë¦¬ìŠ¤í¬ê´€ë¦¬ ë³´ì¡°ì›ì´ë©°, ì¶œë ¥ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤. "
        "ê´‘ê³ /êµ¬ë…/ì €ì‘ê¶Œ/ì¶”ì²œê¸°ì‚¬/ëŒ“ê¸€ ë“± ë¶ˆí•„ìš” í…ìŠ¤íŠ¸ëŠ” ì œê±°í•˜ê³ , "
        "ìš”êµ¬ëœ í•„ë“œë§Œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•œë‹¤."
    )

    messages = [
        {"role": "system", "content": sys_msg},
        {"role": "user", "content": build_prompt(article_text, max_chars=2000)},
    ]
    return client.chat(messages, model=model, temperature=0.2)


# =========================
# Selenium ë“œë¼ì´ë²„ êµ¬ì„±/í•´ì œ
# =========================
def build_driver(download_dir: Optional[Path] = None) -> webdriver.Chrome:
    """
    Selenium Chrome ë“œë¼ì´ë²„ ìƒì„±.
    - í¬ë¡¬ í”„ë¡œí•„ ì¬ì‚¬ìš©(íšŒì‚¬ í”„ë¡ì‹œ/ë³´ì•ˆ ì„¤ì • ê·¸ëŒ€ë¡œ í™œìš©)
    - page_load_timeout ê¸°ë³¸ 45ì´ˆ
    """
    chrome_options = webdriver.ChromeOptions()

    # íŒ€ì¥ë‹˜ PCì— ì„¤ì¹˜ëœ Chrome ê²½ë¡œ ì§€ì •
    if CHROME_PATH.exists():
        chrome_options.binary_location = str(CHROME_PATH)

    # ì§€ì •í•œ ì‚¬ìš©ì í”„ë¡œí•„ ì‚¬ìš©
    chrome_options.add_argument(f'--profile-directory={CHROME_PROFILE}')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--disable-notifications')
    chrome_options.add_argument('--start-maximized')

    # (ì„ íƒ) ë‹¤ìš´ë¡œë“œ ê²½ë¡œ ì§€ì •
    if download_dir:
        prefs = {
            "download.default_directory": str(download_dir),
            "download.prompt_for_download": False,
            "directory_upgrade": True,
            "safebrowsing.enabled": True
        }
        chrome_options.add_experimental_option("prefs", prefs)

    driver = webdriver.Chrome(options=chrome_options)
    driver.set_page_load_timeout(45)  # ğŸ” 30ì´ˆ â†’ 45ì´ˆë¡œ ì—¬ìœ 
    return driver


def safe_quit(driver: webdriver.Chrome):
    """ë“œë¼ì´ë²„ ì¢…ë£Œ ì‹œ ì˜ˆì™¸ ë¬´ì‹œí•˜ê³  ì•ˆì „í•˜ê²Œ ì¢…ë£Œ."""
    try:
        driver.quit()
    except Exception:
        pass


# =========================
# ì—‘ì…€ ìœ í‹¸
# =========================
def ensure_news_column_at_G(df: pd.DataFrame) -> pd.DataFrame:
    """
    'news' ì¹¼ëŸ¼ì„ Gì—´(index=6)ì— ê³ ì •:
    - ì—†ìœ¼ë©´ ìƒì„±í•˜ì—¬ Gì—´ì— ì‚½ì…
    - ìˆìœ¼ë©´ í•´ë‹¹ ì¹¼ëŸ¼ì„ Gì—´ ìœ„ì¹˜ë¡œ ì´ë™
    """
    cols = list(df.columns)

    if "news" not in cols:
        insert_at = min(TARGET_NEWS_COL_INDEX, len(cols))
        df.insert(insert_at, "news", pd.NA)
    else:
        current_idx = cols.index("news")
        if current_idx != TARGET_NEWS_COL_INDEX:
            cols.pop(current_idx)
            insert_at = min(TARGET_NEWS_COL_INDEX, len(cols))
            cols.insert(insert_at, "news")
            df = df.reindex(columns=cols)

    return df


def should_fill(val) -> bool:
    """
    newsë¥¼ ì±„ìš¸ì§€ ì—¬ë¶€(FILL_ONLY_EMPTY ì •ì±…).
    - FILL_ONLY_EMPTY=True: ë¹„ì–´ ìˆì„ ë•Œë§Œ True
    - pandasì˜ <NA>, NaN ë“±ë„ ë¹„ì–´ìˆëŠ” ê²ƒìœ¼ë¡œ ì²˜ë¦¬
    """
    if not FILL_ONLY_EMPTY:
        # í•­ìƒ ë®ì–´ì“°ëŠ” ëª¨ë“œì¼ ë•ŒëŠ” ê·¸ëƒ¥ True
        return True

    # ì–´ë–¤ íƒ€ì…ì´ë“  NaN/<NA>/Noneì´ë©´ True
    if pd.isna(val):
        return True

    # ë¬¸ìì—´ì¸ë° ê³µë°±ë¿ì¸ ê²½ìš°ë„ ë¹„ì–´ìˆëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼
    if isinstance(val, str) and not val.strip():
        return True

    return False


# =========================
# í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ (Selenium)
# =========================
def get_visible_text(driver: webdriver.Chrome, url: str) -> str:
    """
    ì£¼ì–´ì§„ URLì„ ì—´ê³ , document.body.innerTextë¥¼ ê°€ì ¸ì˜´.
    - PAGE_LOAD_WAIT ë™ì•ˆ ì¶”ê°€ ëŒ€ê¸°
    - timeout/ê¸°íƒ€ ì˜ˆì™¸ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
    """
    try:
        print(f"[INFO] í˜ì´ì§€ ì—´ê¸°: {url}")
        driver.get(url)
        time.sleep(PAGE_LOAD_WAIT)

        try:
            text = driver.execute_script(
                "return document.body ? document.body.innerText : '';"
            )
        except Exception:
            try:
                text = driver.find_element(By.TAG_NAME, "body").text
            except NoSuchElementException:
                text = ""

        text = text or ""
        print(f"[DEBUG] ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}")
        return text

    except (WebDriverException, TimeoutException) as e:
        print(f"[ERR] í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {e}")
        return ""


# =========================
# íƒ€ê¹ƒ íŒŒì¼(keyword_news.xlsx) Append + FIFO(ìµœê·¼ 30í–‰ ìœ ì§€)
# =========================
def append_rows_to_keyword_news(
    rows_df: pd.DataFrame,
    target_path: Path,
    max_rows: int = MAX_TARGET_ROWS
) -> None:
    """
    - ê¸°ì¡´ targetì„ ì½ì–´ì˜¤ê³ (rows_old), rows_dfë¥¼ ì•„ë˜ë¡œ ì´ì–´ë¶™ì„
    - ì „ì²´ í–‰ì´ max_rowsë¥¼ ì´ˆê³¼í•˜ë©´ 'ë¨¼ì € ë“¤ì–´ì˜¨ í–‰'ë¶€í„° ì œê±°í•˜ì—¬ ìµœê·¼ max_rowsí–‰ ìœ ì§€
    """
    if target_path.exists():
        try:
            rows_old = pd.read_excel(target_path, engine="openpyxl")
            print(f"[INFO] ê¸°ì¡´ íƒ€ê¹ƒ í–‰ ìˆ˜: {len(rows_old)}")
        except Exception as e:
            print(f"[WARN] íƒ€ê¹ƒ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨, ì´ë²ˆ ì‹¤í–‰ë¶€í„° ìƒˆë¡œ ì‹œì‘: {e}")
            rows_old = pd.DataFrame()
    else:
        print("[INFO] íƒ€ê¹ƒ íŒŒì¼ ìµœì´ˆ ìƒì„± ìƒíƒœ")
        rows_old = pd.DataFrame()

    rows_old = ensure_news_column_at_G(rows_old)
    rows_df = ensure_news_column_at_G(rows_df.copy())

    # ê¸°ì¡´ + ì‹ ê·œë¥¼ ê·¸ëŒ€ë¡œ ì´ì–´ ë¶™ì„ (keyword_news.xlsx ë§ˆì§€ë§‰ë¶€í„° ì´ì–´ì„œ)
    out_df = pd.concat([rows_old, rows_df], ignore_index=True)

    # FIFO: ìµœëŒ€ í–‰ ìˆ˜ ìœ ì§€ (ì•ì—ì„œ ì˜ë¼ë‚´ê³  ìµœì‹  max_rowsë§Œ ë‚¨ê¹€)
    if len(out_df) > max_rows:
        out_df = out_df.iloc[-max_rows:].reset_index(drop=True)

    out_df = ensure_news_column_at_G(out_df)
    out_df.to_excel(target_path, index=False, engine="openpyxl")
    print(f"[OK] ì €ì¥ ì™„ë£Œ(target): {target_path.name} (ì´ {len(out_df)}í–‰, max={max_rows})")


# =========================
# ì†ŒìŠ¤ íŒŒì¼ ì²˜ë¦¬ + 'ì´ë²ˆì— ì²˜ë¦¬ëœ í–‰' ë°˜í™˜
# =========================
def process_source_file_and_collect(
    xlsx_path: Path,
    client: ChatClient,
    driver: webdriver.Chrome
) -> pd.DataFrame:
    """
    - (ì†ŒìŠ¤) keyword_news_bf.xlsx í•œ ê°œë¥¼ ì²˜ë¦¬
    - ì´ë²ˆ ì‹¤í–‰ì—ì„œ newsê°€ ìƒˆë¡œ ì±„ì›Œì§„ í–‰ë§Œ ëª¨ì•„ DataFrameìœ¼ë¡œ ë°˜í™˜(íƒ€ê¹ƒ Append ìš©)
    - ì†ŒìŠ¤ íŒŒì¼ì—ë„ news ë°˜ì˜í•˜ì—¬ ì €ì¥
    """
    try:
        df = pd.read_excel(xlsx_path, engine="openpyxl")
    except Exception as e:
        print(f"[ERR] ì—‘ì…€ ë¡œë”© ì‹¤íŒ¨: {xlsx_path.name} -> {e}")
        return pd.DataFrame()

    if "URL" not in df.columns:
        print(f"[SKIP] 'URL' ì¹¼ëŸ¼ ì—†ìŒ: {xlsx_path.name}")
        return pd.DataFrame()

    df = ensure_news_column_at_G(df)

    updated_indices: List[int] = []

    for idx, row in df.iterrows():
        url = str(row.get("URL", "")).strip()
        news_val = row.get("news", None)
        fill_flag = should_fill(news_val)

        print(f"\n[DEBUG] row={idx}, fill?={fill_flag}, news={repr(news_val)}")
        print(f"[DEBUG] URL={url}")

        if not url.startswith("http"):
            print("[SKIP] http/https ì•„ë‹˜")
            continue

        if not fill_flag:
            print("[SKIP] newsê°€ ì´ë¯¸ ì±„ì›Œì ¸ ìˆìŒ(ë®ì–´ì“°ê¸° ëª¨ë“œ ì•„ë‹˜)")
            continue

        # 1) í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        raw = get_visible_text(driver, url)
        if not raw or len(raw) < 30:
            print(f"[WARN] í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ(len={len(raw)})")
            continue

        # 2) í…ìŠ¤íŠ¸ ì •ì œ
        article_text = clean_text(raw)
        print(f"[DEBUG] ì •ì œ í›„ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(article_text)}")

        # 3) GPT ìš”ì•½
        try:
            summary = summarize_article(client, article_text, model=MODEL_ID)
        except Exception as e:
            print(f"[ERR] GPT ìš”ì•½ ì‹¤íŒ¨ row={idx} -> {e}")
            continue

        # 4) ì†ŒìŠ¤ dfì— news ë°˜ì˜
        df.at[idx, "news"] = summary
        updated_indices.append(idx)
        print(f"[OK] row={idx} ìš”ì•½ ì™„ë£Œ")

        # 5) API í˜¸ì¶œ ê°„ ê°„ê²©
        time.sleep(REQUEST_INTERVAL_SEC)

    # ì†ŒìŠ¤ íŒŒì¼ ì €ì¥
    try:
        df.to_excel(xlsx_path, index=False, engine="openpyxl")
        print(f"\n[OK] ì €ì¥ ì™„ë£Œ(source): {xlsx_path.name} (updated={len(updated_indices)})")
    except Exception as e:
        print(f"[ERR] ì†ŒìŠ¤ ì €ì¥ ì‹¤íŒ¨: {xlsx_path.name} -> {e}")

    if updated_indices:
        # ì´ë²ˆì— ì‹¤ì œë¡œ ìƒˆë¡œ ìš”ì•½ëœ í–‰ë§Œ ë°˜í™˜ (íƒ€ê¹ƒ append ìš©)
        return df.loc[updated_indices].copy()

    return pd.DataFrame()


# =========================
# ì‹¤í–‰ë¶€
# =========================
def main():
    # 1) API í‚¤ ë¡œë”©
    api_key = load_api_key(KEY_PATH)
    client = ChatClient(api_key=api_key)

    # 2) Selenium ë“œë¼ì´ë²„ ìƒì„±
    driver = build_driver(download_dir=None)

    try:
        bf_path = SOURCE_BF

        if not bf_path.exists():
            print(f"[ERR] ì†ŒìŠ¤ íŒŒì¼ ì—†ìŒ: {bf_path}")
            return

        print(f"[INFO] ì†ŒìŠ¤ íŒŒì¼: {bf_path.name}")
        print(f"[INFO] íƒ€ê¹ƒ íŒŒì¼: {TARGET_NEWS.name} (ìµœëŒ€ {MAX_TARGET_ROWS}í–‰, FIFO)")

        # 3) ì†ŒìŠ¤ íŒŒì¼ ì²˜ë¦¬ (keyword_news_bf.xlsxì˜ news ë¹ˆ ì¹¸ ì±„ìš°ê¸°)
        processed_rows = process_source_file_and_collect(bf_path, client, driver)

        # 4) íƒ€ê¹ƒ íŒŒì¼ Append + FIFO ìœ ì§€
        #    - keyword_news.xlsx ê¸°ì¡´ í–‰ ë’¤ì— processed_rows(ì´ë²ˆì— ìƒˆë¡œ ìš”ì•½ëœ í–‰) ì´ì–´ ë¶™ì´ê³ 
        #    - ìƒìœ„ë¶€í„° ì‚­ì œí•´ì„œ ìµœê·¼ ê¸°ì¤€ 30ê°œë§Œ ë‚¨ê¹€
        if not processed_rows.empty:
            append_rows_to_keyword_news(
                processed_rows,
                TARGET_NEWS,
                max_rows=MAX_TARGET_ROWS
            )
        else:
            print("[INFO] ì´ë²ˆ ì‹¤í–‰ì—ì„œ ìƒˆë¡œ ì²˜ë¦¬ëœ í–‰ì´ ì—†ìŠµë‹ˆë‹¤(ìš”ì•½ ëŒ€ìƒ ì—†ìŒ).")

    finally:
        safe_quit(driver)


if __name__ == "__main__":
    main()


